代数表达式和计算图都对**符号**(symbol)或不具有特定值的变量进行操作。
这些代数或者基于图的表达式被称为**符号表示**(symbolic representation)。
当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。
我们用一个特定的**数值**(value)来替代网络的符号输入$x$，例如$[1.2, 3,765, -1.8]^\top$。

一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值。
我们将这种方法称为**符号到数值**的微分。  
> **[success]**  
> ![](/assets/images/Chapter6/10.png)  

这种方法用在诸如Torch和Caffe之类的库中。  
> **[warning]** [?]

另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。
这是Theano和TensorFlow所采用的方法。
图6.10给出了该方法如何工作的一个例子。  
![](/assets/images/Chapter6/11.png)  
> **[info] 使用符号到符号的方法计算导数的示例**  

在这种方法中，反向传播算法不需要访问任何实际的特定数值。
相反，它将节点添加到计算图中来描述如何计算这些导数。
通用图形求值引擎可以在随后计算任何特定数值的导数。
{(左)}在这个例子中，我们从表示$z=f(f(f(w)))$的图开始。
{(右)}我们运行反向传播算法，指导它构造表达式$\frac{dz}{dw}$对应的图。 在这个例子中，我们不解释反向传播算法如何工作。
我们的目的只是说明想要的结果是什么：符号描述的导数的计算图。

这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。
因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数再进行求导就能得到更高阶的导数。
高阶导数的计算在第6.5.10中描述。  

我们将使用后一种方法，并且使用构造导数的计算图的方法来描述反向传播算法。  
> **[success]**  
> 另外一些参数资料上使用第一种方法。所以有些从别的资料上来的截图会和书上画得不一样。  

图的任意子集之后都可以使用特定的数值来求值。
这允许我们避免精确地指明每个操作应该在何时计算。
相反，通用的图计算引擎只要当一个节点的父节点的值都可用时就可以进行求值。  
> **[warning]** 图计算引擎?  
  
基于符号到符号的方法的描述包含了符号到数值的方法。
符号到数值的方法可以理解为执行了与符号到符号的方法中构建图的过程中完全相同的计算。
关键的区别是符号到数值的方法不向外暴露出计算图。
