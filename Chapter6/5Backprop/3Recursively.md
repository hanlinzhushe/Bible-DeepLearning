反向传播的原理：  
由[微积分中的链式法则](https://windmising.gitbook.io/bible-deeplearning/0introduction/0introduction-2/2chainrule)可知：偏导可以表示为多个子项相乘的结果。  
这些子项可能会被计算多次。对于这些子项，是“1. 作为中间结果存储下来”，还是“2.每次都重新计算”？  
选择前者，意味着高内存开销，选择后者，意味着高运行时间。  
反向传播算法选择了前者，牺牲一部分内存，减少运行时间。  

# 书上的一些定义

书上的符号很多，长得还很像，这是我对这些符号的理解。  

## $$u^{(n)}$$

$$u^{(n)}$$是训练样本的损失函数，不是一个Unit。但为了便于理解，假设在原网络的最后面再加上一层，这一层只有一个unit。这个unit上的计算就是对上一层的输出计算cost。  

*如果能想像出来我就不画图了*。  

$$u^{(n)}$$可以看作是新加的一层上的用于计算cost的unit。  

**后面我将使用L代表最后一层，$$u^L$$来代表$$u^{(n)}$$。**  

## $$u^{(i)}$$

输入Unit和隐藏Unit和原网络的输出层unit一共有$$n_i$$个，分别是$$u^{(1)},u^{(2)},\cdots,u^{(n_i)}$$。  

[?]上面$$n_i$$的i是什么意思？
没想明白，不过可以肯定的是下一句话  
> 对所有的$$i \in \{1, 2, \cdots, n_i\}计算\frac{\partial u^{(n)}}{\partial u^{(i)}}$$

中的i肯定和$$n_i$$的不是一回事。  

**后面我将使用$$u^l_{i}$$来代表第$$l$$层的第i个unit。  
因为$$u^{l1}_{i}$$中的i和$$u^{l2}_{i}$$中的i不是一个概念，所以下标也会用j或者其它符号来区分。**  

## $$A^{(i)}$$

目标没有对这些unit的关系作假设，即它们可能是在同一层，也可能在不同层。  
因此就有了$$A^{(i)}$$这个符号，这表示所有在$$u^{(i)}$$左侧的unit的集合。    
还有一个函数$$Pa(u^{(i)})$$在书上没有定义，我理解$$Pa(u^{(i)})$$和$$A^{(i)}$$是一回事。   

**后面我将使用$$u^{<l}$$来表示l层左边的所有unit的集合。**

## $$f^{(i)}$$

> 每个节点通过将函数f(i) 应用到变量集合A(i) 上来计算u(i) 的值

我理解每一层使用函数f应该是一样的，所以我**使用$$f^l$$代表第l层上函数，$$F^l$$代表应用到l及其左边所有层上的函数。**  
即：  
$$
F^1(x) = f^1(x)  \\
F^2(x) = f^2(f^1(x))
$$
以次类推。  
$$F^L$$得到的是最终的cost。

$$F^l$$就是$$u^l$$的输出。  

# $$\frac{\partial u^{(n)}}{\partial u^{(i)}}$$

书上符号$$u^{(n)}$$和$$u^{(i)}$$都有二种含义。    
1. 代表一个unit。这点在上面已经解释过了。例如$$u^{(n)}$$代表unit $$u^L$$  
2. 代表这个神经元的输出。例如$$u^{(n)}$$可以代表unit $$u^L$$的输出，即$$u^{(n)} = F^L$$。    

> 对所有的$$i \in \{1, 2, \cdots, n_i\}计算\frac{\partial u^{(n)}}{\partial u^{(i)}}$$

这里的$$u^{(n)}$$取第二种含义，即$$F^L$$  
这里的$$u^{(i)}$$也取第二种含义，即$$u^{(n)}$$的输出。  
我理解是要求代价函数对神经网络中的每一个unit的输出的偏导。或者说每一个unit的输出的变化对cost的变化的影响。    

**后面我将使用$$\frac{\partial u^L}{\partial u^l_i}$$来代表代价函数对第l层第i个神经元的结果的偏导。**  

# 算法6.1 正向传播算法

已知$$F^L = f^L(f^{L-1}(f^{\cdots}(f^1(x))))$$  
可知，要计算$$F^L$$，先要得到x，并计算$$F^1(x), F^2(x), \cdots$$  
正向传播就是信息从x传递到$$F^L$$的过程。  
同理，在计算$$F^{L-1}$$时也要先$$F^1(x), F^2(x), \cdots$$  

算法6.1想说明的是：  
不需要每次都从$$F^1(x), F^2(x), \cdots$$开始计算。  
可以按照层递进，**沿着正向**一层一层地往前算，每算完一层就把这一层的结果记下来。  
计算下一层时直接在上一层结果的基础上进行计算。  
每一层的中间结果存下来，增加了内存的消耗。  
但避免重复计算，减少了运行时间。  

# 算法6.2 反向传播算法

*Note：中文版中算法6.2的内容跑到6.5.4节去了*

反向传播算法和正向传播算法的思想是一样的。  

根据[微积分中的链式法则](https://windmising.gitbook.io/bible-deeplearning/0introduction/0introduction-2/2chainrule)来计算每个unit的偏导。  

先计算第L-1层的偏导：  
$$
\frac{\partial u^L}{\partial u^{L-1}_i} = (F^L)'(u^{L-1}_i)
$$
再计算第L-2层的偏导：  
$$
\frac{\partial u^L}{\partial u^{L-2}_j} = \sum_i \frac{\partial u^L}{\partial u^{L-1}_i} \frac{\partial u^{L-1}_i}{\partial u^{L-2}_j} 
$$
不需要继续算到第1层了，仅观察第L-1层和L-2层的偏导，就会发现**L-2层的偏导需要用到L-1层的偏导结果**。  
这与计算$$F^L$$的过程是非常相似的。  
只不过这次是信息从代价函数传递到x的过程，因此称为反向传播。  

算法6.2想说明的是：  
不需要每次都从第L-1、L-2、...的偏导开始计算。  
可以按照层递进，**沿着反向**一层一层地往后算，每算完一层就把这一层的结果记下来。  
计算上一层时直接在下一层结果的基础上进行计算。  
每一层的中间结果存下来，增加了内存的消耗。  
但避免重复计算，减少了运行时间。  

![](https://github.com/windmissing/Bible-DeepLearning/raw/master/Chapter6/images/5.png)  