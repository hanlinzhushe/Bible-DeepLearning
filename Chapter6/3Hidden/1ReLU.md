# ReLU

$$
g(z) = max{0, z}
$$

特点：  
1. 易于优化  
2. 只要处于激活状态，导数都能保持比较大  
3. 相比引入二阶效应的激活函数，它的梯度方向对于学习来说更有用。  
4. 可以将b初始化为一个小的正数，例如0.1  
5. 缺点：不激活时无法学习。改进：  
$$
g(z, a_i) = \max(0, z_i) + a_i\min(0, z_i)
$$
zi大于0时用第一项，zi小于0时用第二项。  
(1)ai=-1  
(2)ai=0.01
(3)ai作为学习参数  

# maxout单元

[?]没看懂