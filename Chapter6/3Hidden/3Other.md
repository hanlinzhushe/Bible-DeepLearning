也存在许多其他种类的隐藏单元，但它们并不常用。

一般来说，很多种类的可微函数都表现得很好。
许多未发布的激活函数与流行的激活函数表现得一样好。
为了提供一个具体的例子，作者在MNIST数据集上使用$h=\cos(Wx+b)$测试了一个前馈网络，并获得了小于1\%的误差率，这可以与更为传统的激活函数获得的结果相媲美。
在新技术的研究和开发期间，通常会测试许多不同的激活函数，并且会发现许多标准方法的变体表现非常好。
这意味着，通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。
新的隐藏单元类型如果与已有的隐藏单元表现大致相当的话，那么它们是非常常见的，不会引起别人的兴趣。

> **[success]**
还有很多其它激活函数用作中间层神经元，效果也很好。  
只有效果显著改进的激活函数才会被关注。

列出文献中出现的所有隐藏单元类型是不切实际的。
我们只对一些特别有用和独特的类型进行强调。

其中一种是完全没有激活函数$g(z)$。
也可以认为这是使用单位函数作为激活函数的情况。
我们已经看过线性单元可以用作神经网络的输出。
它也可以用作隐藏单元。
如果神经网络的每一层都仅由线性变换组成，那么网络作为一个整体也将是线性的。
然而，神经网络的一些层是纯线性也是可以接受的。
考虑具有$n$个输入和$p$个输出的神经网络层$h=g(W^\top x+b)$。
我们可以用两层来代替它，一层使用权重矩阵$U$，另一层使用权重矩阵$V$。
如果第一层没有激活函数，那么我们（用$U$和$V$替代$W$）实际上是对基于$W$的原始层的权重矩阵进行了因式分解。
分解方法是计算$h=g(V^\top U^\top x+b)$。
如果$U$产生了$q$个输出，那么$U$和$V$一起仅包含$(n+p)q$个参数，而$W$包含$np$个参数。
如果$q$很小，这可以在很大程度上节省参数。
这是以将线性变换约束为低秩的代价来实现的，但这些低秩关系往往是足够的。
线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。  
> **[success]**  
没有激活函数。g(z) = z  
提供了一种减少网络中参数数量的有效方法。  
例如size=[in, out]，需要in*out个参数  
增加一层后为size=[in, mid, out]，需要(in+out)*mid个参数，如果mid非常小，需要的参数个数就会变少。 


softmax单元是另外一种经常用作输出的单元（如\sec?中所描述的），但有时也可以用作隐藏单元。  
> **[success]**  
[softmax](https://windmising.gitbook.io/bible-deeplearning/0introduction/0introduction/0introduction-1/3softmax) 
softmax单元很自然地表示具有$k$个可能值的离散型随机变量的概率分布，所以它们可以用作一种开关。
这些类型的隐藏单元通常仅用于明确地学习操作内存的高级结构中，将在第10.12节中描述。
> **[warning]** 第10.12节关于softmax的部分没看懂

其他一些常见的隐藏单元类型包括：

+ 径向基函数：$h_i = \exp \left (-\frac{1}{\sigma_i^2}|| W_{:,i}-x||^2 \right )$。
这个函数在$x$接近模板$W_{:,i}$时更加活跃。
因为它对大部分$x$都饱和到0，因此很难优化。

+ \textbf{softplus}函数：$g(a)=\zeta(a)=\log(1+e^a)$。
这是整流线性单元的平滑版本，由{Dugas01}引入用于函数近似，由{Nair-2010-small}引入用于无向概率模型的条件分布。
{Glorot+al-AI-2011-small}比较了softplus和整流线性单元，发现后者的结果更好。
通常不鼓励使用softplus函数。
softplus表明隐藏单元类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。

+ 硬双曲正切函数：它的形状和$\text{tanh}$以及整流线性单元类似，但是不同于后者，它是有界的，$g(a)=\max(-1, \min(1,a))$。
它由{Collobert04}引入。


隐藏单元的设计仍然是一个活跃的研究领域，许多有用的隐藏单元类型仍有待发现。
