深度前前馈网络 deep forward network

深度前馈网络的目标：  
近似某个函数f*(x)  
为什么称为“前馈”？  
因为信息只从前向后传，不存在从后向前。如果有从后前向传递信息，就称为循环（recurrent）神经网络。  

# 一些术语

网络 network：称为网络是因为它由许多不同的函数组装到一起。例如$$f^{(1)}$$、$$f^{(2)}$$、$$f^{(3)}$$，连到一起形成了$$f^{(3)}(f^{(2)}(f^{(1)}(x)))$$  
第一层 first layer：例子中的$$f^{(1)}$$  
第二层 second layer：例子中的$$f^{(2)}$$  
深度 depth：函数链的长度。  
输出层 output layer：前馈网络的最后一层。  
f*(x)：要近似的函数。  
f(x)：由模型得到的函数。  
隐藏层 hidden layer：中间层的结果不会展示出来，因为称为“隐藏层”。  
宽度 width：中间层是以向量的形式表示的。向量的维度即模型的宽度。  
神经元 neuron：中间层向量里面的每一个元素可以看作是一个神经元。  
层 layer：两个解释表达同一个意思（1）一个“向量->向量”的函数。（2）一组可以并行计算的单元，每个单元是一个“向量->标量”的函数。每个单元从许多其它单元获取输入并得到一个激活值。这些单元形象化地称为“神经元”。  

# 从线性模型开始

以逻辑回归、线性回归为代表的线性模型  
优点：无论是通过[闭解形式](https://windmising.gitbook.io/mathematics-basic-for-ml/gao-deng-shu-xue/function)还是使用凸优化，它们都能高效且可靠地拟合。  
缺点：无法理解任何两个输入变量间的相互作用。  

改进方法：不把线性模型用于x 本身，而是用在一个变换后的输入ϕ(x) 上。  
其中：ϕ是一种非线性变化，或核技巧。  

# 怎样选择ϕ？

1. 使用一个通用的ϕ，例如RBF核中所使用的“无限维ϕ”。  
优点：当ϕ的维度足够高，可以拟合任何训练数据。  
缺点：泛化能力不够。  
2. 手动设计ϕ。  
3. 让机器自己学习ϕ。  
令：
$$
y = f(x;\theta, w) = \phi(x; \theta)^Tw。
$$
其中，w为$$\phi(x)$$到输出的映射。$$\theta$$用于从一大类函数中学习$$\phi$$的参数。$$\phi$$定义了一个隐藏层。  
在这种方法中，人为设计一个通常函数族，机器学习最优的$$\theta$$，共同得到$$\phi$$。  

# 本章内容

1. 一个简单的前馈网络  
2. 网络的各个细节  
3. 设计网络的架构  
4. 反向传播算法  
5. 一些历史观点