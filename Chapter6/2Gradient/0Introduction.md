使用梯度下降法训练神经网络与训练其它模型类似。  
但是在神经网络中，损失函数通常是非凸，因此不一定能找到最小的值。  

在非凸函数中，梯度下降法的结果与初始化值有关，关于参数的初始化见第8章。  
梯度下降法及其改进见4.5。  
随机梯度下降法见5.9。  
使用反向传播算法快速计算神经网络的梯度，见6.5。  