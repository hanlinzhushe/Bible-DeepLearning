机器学习的目标函数可以分解成训练样本上的求和，仅使用一部分样本来估计整体样本的期望。  

# 为什么要小批量？  

1. 快速地计算梯度的估计值，而不是缓慢地计算估计的准确值。  
2. 训练集的冗余，大量样本对梯度做了相似的贡献。   
3. 可以并行计算

# size的选择

选择不同的size得到的算法各有名字。  

|size的大小 | 算法的名字|
|---|---|
|size=样本数|批量（batch）或确定性（deterministic）梯度算法|
|size=1|随机（stochastic）或者在线（on-line）算法|
|$$size \in (1, 样本数)$$|为小批量（minibatch）或小批量随机（minibatch stochastic）方法|

size大小需要考虑的因素：  
1. size太大计算量很大，但回报（标准差降低）没那么多。  
2. size太小难以充分利用多核架构，整体训练时间会很长。  
3. 内存消耗和size会正比。  
4. 在GPU上，使用2 的幂数作为size可以获得更少的运行时间。  
5. size=1时泛化效果最好。  

[?]有些算法不合适小批量？例如Hessian矩阵？

# 样本的抽取

1. 两个连续的小批量样本应该是独立的。  
2. 先全部打乱样本，然后按照打乱后的顺序依次取样本。  
3. 不打乱样本可能是有害的。  

[?]讲泛化的那几段没看懂。  
