# 别名  
权重衰减，weight decay，L2参数惩罚，岭回归，Tikhonov 正则  

# 代价函数

*为简化内容，将b省略*。  

$$
\begin{eqnarray}
\Omega(\theta) & = & \frac{1}{2}||w||^2_2  \tag {1}\\
\tilde J(w;X,y) & = & \frac{\alpha}{2}w^Tw + J(w;X,y) \tag {2}\\
\nabla_w \tilde J(w;X,y) & = & \alpha w + \nabla_w J(w;X,y) \tag {3}\\
w \rightarrow w' & = & w - \epsilon(\alpha w + \nabla_w J(w;X,y))\\
& = & (1-\epsilon\alpha)w - \epsilon \nabla_w J(w;X,y)\tag {4}
\end{eqnarray}
$$

公式（4）中的$$(1-\epsilon\alpha)w$$就是权重衰减。  

# 数学意义  

[?]中间推导没看懂  
最后结论是：  
1. 权重衰减对优化一个抽象通用的二次代价函数的影响是“**只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向（对应Hessian 矩阵较小的特征值）上对应的分量会在训练过程中因正则化而衰减掉。**”
2. 权重衰减对优化一个真实的二次代价函数$$\tilde J(\theta;X,y)$$的影响是“**L2正则化能让学习算法"感知"到具有较高方差的输入x，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。**”