# 选择超参数  

大部分深度学习算法都有许多超参数来控制不同方面的算法表现。
有些超参数会影响算法运行的时间和存储成本。
有些超参数会影响学习到的模型质量，以及在新输入上推断正确结果的能力。

有两种选择超参数的基本方法：手动选择和自动选择。
**手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。
自动选择超参数算法大大减少了解这些想法的需要，但它们往往需要更高的计算成本。**  
> **[success]**  
> 讲了好多，单纯看懂没有什么难度。但要真正训练过模型才会有比较深刻的理解  



## 手动调整超参数
手动设置超参数，我们必须了解超参数、训练误差、泛化误差和计算资源（内存和运行时间）之间的关系。
这需要切实了解一个学习算法有效容量的基础概念，如\chap?所描述的。
<!-- % 416 mid -->


手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差。
我们不去探讨如何确定各种超参数对运行时间和内存的影响，因为这高度依赖于平台。


手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂性。
有效容量受限于三个因素：模型的表示容量、学习算法成功最小化训练模型代价函数的能力以及代价函数和训练过程正则化模型的程度。
具有更多网络层，每层有更多隐藏单元的模型具有较高的表示能力——能够表示更复杂的函数。
然而，如果训练算法不能找到某个合适的函数来最小化训练代价，或是正则化项（如权重衰减）排除了这些合适的函数，那么即使模型的表达能力较高，也不能学习出合适的函数。%??  还是太乱

<!-- % 416 mid -->


当泛化误差以某个超参数为变量，作为函数绘制出来时，通常会表现为U形曲线，如\fig?所示。
在某个极端情况下，超参数对应着低容量，并且泛化误差由于训练误差较大而很高。
这便是欠拟合的情况。
另一种极端情况，超参数对应着高容量，并且泛化误差由于训练误差和测试误差之间的差距较大而很高。
最优的模型容量位于曲线中间的某个位置，能够达到最低可能的泛化误差，由某个中等的泛化差距(generalization gap)和某个中等的训练误差相加构成。
<!-- % -- 416 end -->



对于某些超参数，当超参数数值太大时，会发生过拟合。
例如中间层隐藏单元的数量，增加数量能提高模型的容量，容易发生过拟合。%??  原文不符 
对于某些超参数，当超参数数值太小时，也会发生过拟合。
例如，最小的权重衰减系数允许为零，此时学习算法具有最大的有效容量，反而容易过拟合。%??  原文不符   这一段好像偏差较大 
<!-- % 417 head -->


并非每个超参数都能对应着完整的U形曲线。
很多超参数是离散的，如中间层单元数目或是~maxout单元中线性元件的数目，这种情况只能沿曲线探索一些点。
有些超参数是二值的。
通常这些超参数用来指定是否使用学习算法中的一些可选部分，如预处理步骤减去均值并除以标准差来标准化输入特征。
这些超参数只能探索曲线上的两点。
其他一些超参数可能会有最小值或最大值，限制其探索曲线的某些部分。%??    
例如，权重衰减系数最小是零。
这意味着，如果权重衰减系数为零时模型欠拟合，那么我们将无法通过修改权重衰减系数探索过拟合区域。
换言之，有些超参数只能减少模型容量。
<!-- % 417 mid -->


学习率可能是最重要的超参数。
如果你只有时间调整一个超参数，那就调整学习率。
相比其他超参数，它以一种更复杂的方式控制模型的有效容量——当学习率适合优化问题时，模型的有效容量最高，此时学习率是\emph{正确}的，既不是特别大也不是特别小。
学习率关于\emph{训练误差}具有U形曲线，如\fig?所示。
当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。
在理想化的二次情况下，如果学习率是最佳值的两倍大时，会发生这种情况{cite?}。
当学习率太小，训练不仅慢，还有可能永久停留在一个很高的训练误差。
关于这种效应，我们知之甚少（不会发生于一个凸损失函数中）。
<!-- % 417 end -->

\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter11/figures/lr_color}}
\fi
\caption{训练误差和学习率之间的典型关系。
注意当学习率大于最优值时误差会有显著的提升。
此图针对固定的训练时间，越小的学习率有时候可以以一个正比于学习率减小量的因素来减慢训练过程。
泛化误差也会得到类似的曲线，或者由于学习率过大或过小引起的正则化作用而变得复杂化。
由于一个糟糕的优化从某种程度上说可以避免过拟合，即使是训练误差相同的点也会拥有完全不同的泛化误差。}
\end{figure}

<!-- % 417 end -->
调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判断模型是否过拟合或欠拟合，然后适当调整其容量。


如果训练集错误率大于目标错误率，那么只能增加模型容量以改进模型。
如果没有使用正则化，并且确信优化算法正确运行，那么有必要添加更多的网络层或隐藏单元。
然而，令人遗憾的是，这增加了模型的计算代价。
<!-- % 418 head -->


如果测试集错误率大于目标错误率，那么可以采取两个方法。
测试误差是训练误差和测试误差之间差距与训练误差的总和。
寻找最佳的测试误差需要权衡这些数值。
当训练误差较小（因此容量较大），测试误差主要取决于训练误差和测试误差之间的差距时，通常神经网络效果最好。
此时目标是缩小这一差距，使训练误差的增长速率不快于差距减小的速率。
要减少这个差距，我们可以改变正则化超参数，以减少有效的模型容量，如添加~Dropout~或权重衰减策略。
通常，最佳性能来自正则化得很好的大规模模型，比如使用~Dropout~的神经网络。
<!-- % 418 mid -->


大部分超参数可以通过推理其是否增加或减少模型容量来设置。
部分示例如表\?所示。


<!-- % 418 end -->
手动调整超参数时，不要忘记最终目标：提升测试集性能。
加入正则化只是实现这个目标的一种方法。
只要训练误差低，随时都可以通过收集更多的训练数据来减少泛化误差。
实践中能够确保学习有效的暴力方法就是不断提高模型容量和训练集的大小，直到解决问题。
这种做法增加了训练和推断的计算代价，所以只有在拥有足够资源时才是可行的。
原则上，这种做法可能会因为优化难度提高而失败，但对于许多问题而言，优化似乎并没有成为一个显著的障碍，当然，前提是选择了合适的模型。
<!-- % 419 end -->


<!-- % -- 419 -->
\begin{table}
\centering
\small
\begin{tabular}{p{2.5cm}|p{1.5cm}|p{4.0cm}|p{4.0cm}}
超参数 & 容量何时增加 & 原因  & 注意事项 \\
\hline
隐藏单元数量 &  增加          & 增加隐藏单元数量会增加模型的表示能力。 & 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加。\\
\hline
学习率 & 调至最优 & 不正确的学习速率，不管是太高还是太低都会由于优化失败而导致低有效容量的模型。
 & \\
\hline
卷积核宽度 & 增加 & 增加卷积核宽度会增加模型的参数数量。&
较宽的卷积核导致较窄的输出尺寸，除非使用隐式零填充减少此影响，否则会降低模型容量。 
较宽的卷积核需要更多的内存存储参数，并会增加运行时间，但较窄的输出会降低内存代价。
\\
\hline
隐式零填充 & 增加 & 在卷积之前隐式添加零能保持较大尺寸的表示。&
大多数操作的时间和内存代价会增加。\\
\hline
权重衰减系数 & 降低 & 降低权重衰减系数使得模型参数可以自由地变大。
 & \\
\hline
Dropout\,比率 & 降低 & 较少地丢弃单元可以更多地让单元彼此"协力"来适应训练集。
 & \\
\end{tabular}
\caption{各种超参数对模型容量的影响。}
\index{Dropout}
\index{Weight decay}
\end{table}
<!-- % -- 419 -->




## 自动超参数优化算法n
理想的学习算法应该是只需要输入一个数据集，就可以输出学习的函数，而不需要手动调整超参数。
一些流行的学习算法，如逻辑回归和支持向量机，流行的部分原因是这类算法只有一到两个超参数需要调整，它们也能表现出不错的性能。
有些情况下，所需调整的超参数数量较少时，神经网络可以表现出不错的性能；但超参数数量有几十甚至更多时，效果会提升得更加明显。
当使用者有一个很好的初始值，例如由在相同类型的应用和架构上具有经验的人确定初始值，或者使用者在相似问题上具有几个月甚至几年的神经网络超参数调整经验，那么手动调整超参数能有很好的效果。%??  由在相同类型的应用和架构上具有经验的人确定初始  这句话好像不对吧？ --没觉得有问题，你觉得是什么了
然而，对于很多应用而言，这些起点都不可用。
在这些情况下，自动算法可以找到合适的超参数。
<!-- % 420 head -->


如果我们仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这其实是一种优化：
我们在试图寻找超参数来优化目标函数，例如验证误差，有时还会有一些约束（如训练时间，内存或识别时间的预算）。
因此，原则上有可能开发出封装学习算法的超参数优化算法，并选择其超参数，从而使用者不需要指定学习算法的超参数。
令人遗憾的是，超参数优化算法往往有自己的超参数，如学习算法的每个超参数应该被探索的值的范围。
然而，这些次级超参数通常很容易选择，这是说，相同的次级超参数能够很多不同的问题上具有良好的性能。
<!-- % 420 mid -->


## 网格搜索n
当有三个或更少的超参数时，常见的超参数搜索方法是网格搜索。
对于每个超参数，使用者选择一个较小的有限值集去探索。
然后，这些超参数笛卡尔乘积得到一组组超参数，网格搜索使用每组超参数训练模型。
挑选验证集误差最小的超参数作为最好的超参数。
如\fig?所示超参数值的网格。
<!-- % -- 420 end -->


\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\begin{tabular}{cc}
\includegraphics[width=0.35\textwidth]{Chapter11/figures/grid} &
\includegraphics[width=0.35\textwidth]{Chapter11/figures/random}
\end{tabular}
\fi
\caption{网格搜索和随机搜索的比较。
为了方便地说明，我们只展示两个超参数的例子，但是我们关注的问题中超参数个数通常会更多。
\emph{(左)}为了实现网格搜索，我们为每个超参数提供了一个值的集合。
搜索算法对每一种在这些集合的交叉积中的超参数组合进行训练。
\emph{(右)}为了实现随机搜索，我们给联合超参数赋予了一个概率分布。
通常超参数之间是相互独立的。
常见的这种分布的选择是均匀分布或者是对数均匀（从对数均匀分布中抽样，就是对从均匀分布中抽取的样本进行指数运算）的。
然后这些搜索算法从联合的超参数空间中采样，然后运行每一个样本。
网格搜索和随机搜索都运行了验证集上的误差并返回了最优的解。
这个图说明了通常只有一个超参数对结果有着重要的影响。
在这个例子中，只有水平轴上的超参数对结果有重要的作用。
网格搜索将大量的计算浪费在了指数量级的对结果无影响的超参数中，相比之下随机搜索几乎每次测试都测试了对结果有影响的每个超参数的独一无二的值。
此图经~{Bergstra+Bengio-LW2011}允许转载。}
\end{figure}


应该如何选择搜索集合的范围呢？
在超参数是数值（有序）的情况下，每个列表的最小和最大的元素可以基于先前相似实验的经验保守地挑选出来，以确保最优解非常可能在所选范围内。
通常，网格搜索大约会在对数尺度下挑选合适的值，例如，一个学习率的取值集合是$\{0.1,0.01,10^{-3},10^{-4},10^{-5}\}$，或者隐藏单元数目的取值集合$\{50,100,200,500,1000,2000\}$。
<!-- % 421 head -->


通常重复进行网格搜索时，效果会最好。
例如，假设我们在集合$\{-1,0,1\}$上网格搜索超参数 $\alpha$。
如果找到的最佳值是$1$，那么说明我们低估了最优值$\alpha$所在的范围，应该改变搜索格点，例如在集合$\{1,2,3\}$中搜索。
如果最佳值是$0$，那么我们不妨通过细化搜索范围以改进估计，在集合$\{-0.1,0,0.1\}$上进行网格搜索。
<!-- % -- 422 head -->


网格搜索带来的一个明显问题是，计算代价会随着超参数数量呈指数级增长。
如果有$m$个超参数，每个最多取$n$个值，那么训练和估计所需的试验数将是$O(n^m)$。
我们可以并行地进行实验，并且并行要求十分宽松（进行不同搜索的机器之间几乎没有必要进行通信）。
令人遗憾的是，由于网格搜索指数级增长的计算代价，即使是并行，我们也无法提供令人满意的搜索规模。


## 随机搜索n
幸运的是，有一个替代网格搜索的方法，并且编程简单，使用更方便，能更快地收敛到超参数的良好取值：随机搜索~{cite?}。
<!-- % 422 -->


随机搜索过程如下。
首先，我们为每个超参数定义一个边缘分布，例如，Bernoulli分布或范畴分布（分别对应着二元超参数或离散超参数），或者对数尺度上的均匀分布（对应着正实值超参数）。
例如，
\begin{align}
	\texttt{log\_learning\_rate} &\sim u(-1, -5), \\
	\texttt{learning\_rate} &= 10^{\texttt{log\_learning\_rate}},
\end{align}
其中，$u(a,b)$表示区间$(a,b)$上均匀采样的样本。
类似地，$\texttt{log\_number\_of\_hidden\_units}$可以从$u(\log(50), \log(2000))$上采样。
<!-- % 422 mid -->


与网格搜索不同，我们\emph{不需要离散化}超参数的值。这允许我们在一个更大的集合上进行搜索，而不产生额外的计算代价。%?? bin 不知道如何翻译bin，你有什么好的翻译没 
实际上，如\fig?所示，当有几个超参数对性能度量没有显著影响时，随机搜索相比于网格搜索指数级地高效。
{Bergstra+Bengio-2012-small}进行了详细的研究并发现相比于网格搜索， 随机搜索能够更快地减小验证集误差（就每个模型运行的试验数而言）。
<!-- % 422 end -->

与网格搜索一样，我们通常会重复运行不同版本的随机搜索，以基于前一次运行的结果改进下一次搜索。
<!-- % 422 end -->


随机搜索能比网格搜索更快地找到良好超参数的原因是，没有浪费的实验，不像网格搜索有时会对一个超参数的两个不同值（给定其他超参数值不变）给出相同结果。
在网格搜索中，其他超参数将在这两次实验中拥有相同的值，而在随机搜索中，它们通常会具有不同的值。
因此，如果这两个值的变化所对应的验证集误差没有明显区别的话，网格搜索会没有必要地重复两个等价的实验，而随机搜索仍然会对其他超参数进行两次独立地探索。