# 选择超参数  

大部分深度学习算法都有许多超参数来控制不同方面的算法表现。
有些超参数会影响算法运行的时间和存储成本。
有些超参数会影响学习到的模型质量，以及在新输入上推断正确结果的能力。

有两种选择超参数的基本方法：手动选择和自动选择。
**手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。
自动选择超参数算法大大减少了解这些想法的需要，但它们往往需要更高的计算成本。**  
> **[success]**  
> 讲了好多，单纯看懂没有什么难度。但要真正训练过模型才会有比较深刻的理解  








## 自动超参数优化算法n
理想的学习算法应该是只需要输入一个数据集，就可以输出学习的函数，而不需要手动调整超参数。
一些流行的学习算法，如逻辑回归和支持向量机，流行的部分原因是这类算法只有一到两个超参数需要调整，它们也能表现出不错的性能。
有些情况下，所需调整的超参数数量较少时，神经网络可以表现出不错的性能；但超参数数量有几十甚至更多时，效果会提升得更加明显。
当使用者有一个很好的初始值，例如由在相同类型的应用和架构上具有经验的人确定初始值，或者使用者在相似问题上具有几个月甚至几年的神经网络超参数调整经验，那么手动调整超参数能有很好的效果。%??  由在相同类型的应用和架构上具有经验的人确定初始  这句话好像不对吧？ --没觉得有问题，你觉得是什么了
然而，对于很多应用而言，这些起点都不可用。
在这些情况下，自动算法可以找到合适的超参数。
<!-- % 420 head -->


如果我们仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这其实是一种优化：
我们在试图寻找超参数来优化目标函数，例如验证误差，有时还会有一些约束（如训练时间，内存或识别时间的预算）。
因此，原则上有可能开发出封装学习算法的超参数优化算法，并选择其超参数，从而使用者不需要指定学习算法的超参数。
令人遗憾的是，超参数优化算法往往有自己的超参数，如学习算法的每个超参数应该被探索的值的范围。
然而，这些次级超参数通常很容易选择，这是说，相同的次级超参数能够很多不同的问题上具有良好的性能。

