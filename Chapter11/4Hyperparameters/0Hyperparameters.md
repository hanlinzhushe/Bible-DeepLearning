# 选择超参数  

大部分深度学习算法都有许多超参数来控制不同方面的算法表现。
有些超参数会影响算法运行的时间和存储成本。
有些超参数会影响学习到的模型质量，以及在新输入上推断正确结果的能力。

有两种选择超参数的基本方法：手动选择和自动选择。
**手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。
自动选择超参数算法大大减少了解这些想法的需要，但它们往往需要更高的计算成本。**  
> **[success]**  
> 讲了好多，单纯看懂没有什么难度。但要真正训练过模型才会有比较深刻的理解  








## 自动超参数优化算法n
理想的学习算法应该是只需要输入一个数据集，就可以输出学习的函数，而不需要手动调整超参数。
一些流行的学习算法，如逻辑回归和支持向量机，流行的部分原因是这类算法只有一到两个超参数需要调整，它们也能表现出不错的性能。
有些情况下，所需调整的超参数数量较少时，神经网络可以表现出不错的性能；但超参数数量有几十甚至更多时，效果会提升得更加明显。
当使用者有一个很好的初始值，例如由在相同类型的应用和架构上具有经验的人确定初始值，或者使用者在相似问题上具有几个月甚至几年的神经网络超参数调整经验，那么手动调整超参数能有很好的效果。%??  由在相同类型的应用和架构上具有经验的人确定初始  这句话好像不对吧？ --没觉得有问题，你觉得是什么了
然而，对于很多应用而言，这些起点都不可用。
在这些情况下，自动算法可以找到合适的超参数。
<!-- % 420 head -->


如果我们仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这其实是一种优化：
我们在试图寻找超参数来优化目标函数，例如验证误差，有时还会有一些约束（如训练时间，内存或识别时间的预算）。
因此，原则上有可能开发出封装学习算法的超参数优化算法，并选择其超参数，从而使用者不需要指定学习算法的超参数。
令人遗憾的是，超参数优化算法往往有自己的超参数，如学习算法的每个超参数应该被探索的值的范围。
然而，这些次级超参数通常很容易选择，这是说，相同的次级超参数能够很多不同的问题上具有良好的性能。
<!-- % 420 mid -->


## 网格搜索n
当有三个或更少的超参数时，常见的超参数搜索方法是网格搜索。
对于每个超参数，使用者选择一个较小的有限值集去探索。
然后，这些超参数笛卡尔乘积得到一组组超参数，网格搜索使用每组超参数训练模型。
挑选验证集误差最小的超参数作为最好的超参数。
如\fig?所示超参数值的网格。
<!-- % -- 420 end -->


\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\begin{tabular}{cc}
\includegraphics[width=0.35\textwidth]{Chapter11/figures/grid} &
\includegraphics[width=0.35\textwidth]{Chapter11/figures/random}
\end{tabular}
\fi
\caption{网格搜索和随机搜索的比较。
为了方便地说明，我们只展示两个超参数的例子，但是我们关注的问题中超参数个数通常会更多。
\emph{(左)}为了实现网格搜索，我们为每个超参数提供了一个值的集合。
搜索算法对每一种在这些集合的交叉积中的超参数组合进行训练。
\emph{(右)}为了实现随机搜索，我们给联合超参数赋予了一个概率分布。
通常超参数之间是相互独立的。
常见的这种分布的选择是均匀分布或者是对数均匀（从对数均匀分布中抽样，就是对从均匀分布中抽取的样本进行指数运算）的。
然后这些搜索算法从联合的超参数空间中采样，然后运行每一个样本。
网格搜索和随机搜索都运行了验证集上的误差并返回了最优的解。
这个图说明了通常只有一个超参数对结果有着重要的影响。
在这个例子中，只有水平轴上的超参数对结果有重要的作用。
网格搜索将大量的计算浪费在了指数量级的对结果无影响的超参数中，相比之下随机搜索几乎每次测试都测试了对结果有影响的每个超参数的独一无二的值。
此图经~{Bergstra+Bengio-LW2011}允许转载。}
\end{figure}


应该如何选择搜索集合的范围呢？
在超参数是数值（有序）的情况下，每个列表的最小和最大的元素可以基于先前相似实验的经验保守地挑选出来，以确保最优解非常可能在所选范围内。
通常，网格搜索大约会在对数尺度下挑选合适的值，例如，一个学习率的取值集合是$\{0.1,0.01,10^{-3},10^{-4},10^{-5}\}$，或者隐藏单元数目的取值集合$\{50,100,200,500,1000,2000\}$。
<!-- % 421 head -->


通常重复进行网格搜索时，效果会最好。
例如，假设我们在集合$\{-1,0,1\}$上网格搜索超参数 $\alpha$。
如果找到的最佳值是$1$，那么说明我们低估了最优值$\alpha$所在的范围，应该改变搜索格点，例如在集合$\{1,2,3\}$中搜索。
如果最佳值是$0$，那么我们不妨通过细化搜索范围以改进估计，在集合$\{-0.1,0,0.1\}$上进行网格搜索。
<!-- % -- 422 head -->


网格搜索带来的一个明显问题是，计算代价会随着超参数数量呈指数级增长。
如果有$m$个超参数，每个最多取$n$个值，那么训练和估计所需的试验数将是$O(n^m)$。
我们可以并行地进行实验，并且并行要求十分宽松（进行不同搜索的机器之间几乎没有必要进行通信）。
令人遗憾的是，由于网格搜索指数级增长的计算代价，即使是并行，我们也无法提供令人满意的搜索规模。


## 随机搜索n
幸运的是，有一个替代网格搜索的方法，并且编程简单，使用更方便，能更快地收敛到超参数的良好取值：随机搜索~{cite?}。
<!-- % 422 -->


随机搜索过程如下。
首先，我们为每个超参数定义一个边缘分布，例如，Bernoulli分布或范畴分布（分别对应着二元超参数或离散超参数），或者对数尺度上的均匀分布（对应着正实值超参数）。
例如，
\begin{aligned}
	\texttt{log\_learning\_rate} &\sim u(-1, -5), \\
	\texttt{learning\_rate} &= 10^{\texttt{log\_learning\_rate}},
\end{aligned}
其中，$u(a,b)$表示区间$(a,b)$上均匀采样的样本。
类似地，$\texttt{log\_number\_of\_hidden\_units}$可以从$u(\log(50), \log(2000))$上采样。
<!-- % 422 mid -->


与网格搜索不同，我们\emph{不需要离散化}超参数的值。这允许我们在一个更大的集合上进行搜索，而不产生额外的计算代价。%?? bin 不知道如何翻译bin，你有什么好的翻译没 
实际上，如\fig?所示，当有几个超参数对性能度量没有显著影响时，随机搜索相比于网格搜索指数级地高效。
{Bergstra+Bengio-2012-small}进行了详细的研究并发现相比于网格搜索， 随机搜索能够更快地减小验证集误差（就每个模型运行的试验数而言）。
<!-- % 422 end -->

与网格搜索一样，我们通常会重复运行不同版本的随机搜索，以基于前一次运行的结果改进下一次搜索。
<!-- % 422 end -->


随机搜索能比网格搜索更快地找到良好超参数的原因是，没有浪费的实验，不像网格搜索有时会对一个超参数的两个不同值（给定其他超参数值不变）给出相同结果。
在网格搜索中，其他超参数将在这两次实验中拥有相同的值，而在随机搜索中，它们通常会具有不同的值。
因此，如果这两个值的变化所对应的验证集误差没有明显区别的话，网格搜索会没有必要地重复两个等价的实验，而随机搜索仍然会对其他超参数进行两次独立地探索。