# Surrogate Loss Functions 代理损失函数

> **[success] 代理损失函数的优点：**  
（1）可以解决原损失函数不可导的问题  
（2）能估计“测试样本类别”的条件概率。  
（3）改进分类器的鲁棒性  

有时，我们真正关心的损失函数（比如分类误差）并不能被高效地优化。
例如，即使对于线性分类器而言，精确地最小化$0-1$损失通常是不可解的（复杂度是输入维数的指数级别）{cite?}。
在这种情况下，我们通常会优化代理损失函数。
代理损失函数作为原目标的代理，还具备一些优点。  
例如，**正确类别的负对数似然通常用作$0-1$损失的替代**。
负对数似然允许模型估计给定样本的类别的条件概率，如果该模型效果好，那么它能够输出期望最小分类误差所对应的类别。  
> **[warning]**  
条件概率P(y|x)和根据x预测y是什么区别？  

在某些情况下，代理损失函数比原函数学到的更多。
例如，使用对数似然替代函数时，在训练集上的$0-1$损失达到$0$之后，测试集上的$0-1$损失还能持续下降很长一段时间。
这是因为即使$0-1$损失期望是零时，我们还能拉开不同类别的距离以改进分类器的鲁棒性，获得一个更强壮的、更值得信赖的分类器，从而，相对于简单地最小化训练集上的平均$0-1$损失，它能够从训练数据中抽取更多信息。
<!-- % 270 head -->

# Early Stopping 提前终止   

一般的优化和我们用于训练算法的优化有一个重要不同：训练算法通常不会停止在局部极小点。
反之，机器学习通常最小化代理损失函数，但是在基于提前终止（第7.8节）的收敛条件满足时停止。  
> **[success]** [提前终止](https://windmissing.github.io/Bible-DeepLearning/Chapter7/8EarlyStopping.html)  

通常，提前终止使用真实潜在损失函数，如验证集上的$0-1$损失，并设计为在过拟合发生之前终止。
与纯优化不同的是，提前终止时代理损失函数仍然有较大的导数，而纯优化终止时过程已经收敛，导数较小。