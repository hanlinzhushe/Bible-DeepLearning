Polyak平均{cite?}会**平均优化算法在参数空间访问轨迹中的几个点**。
如果$t$次迭代梯度下降访问了点$\theta^{(1)},\cdots,\theta^{(t)}$，那么Polyak平均算法的输出是$\hat{\theta}^{(t)} = \frac{1}{t} \sum_i \theta^{(i)}$。  
> **[success]**  
这是凸问题的更新公式  

在某些问题中，如梯度下降应用于凸问题时，这种方法具有较强的收敛保证。
当应用于神经网络时，其验证更多是启发式的，但在实践中表现良好。
基本想法是，优化算法可能会来回穿过山谷好几次而没经过山谷底部附近的点。
尽管两边所有位置的均值应比较接近谷底。

在非凸问题中，优化轨迹的路径可以非常复杂，并且经过了许多不同的区域。
包括参数空间中遥远过去的点，可能与当前点在代价函数上相隔很大的障碍，看上去不像一个有用的行为。
其结果是，当应用Polyak平均于非凸问题时，通常会使用指数衰减计算平均值：  
$$
\begin{aligned}
    \hat{\theta}^{(t)} = \alpha \hat{\theta}^{(t-1)} + (1-\alpha) \theta^{(t)} .
\end{aligned}
$$

> **[success]**  
这是非凸问题的更新公式  

这个计算平均值的方法被用于大量数值应用中。最近的例子请查看{Szegedy-et-al-2015}。  

> **[warning]**  不懂，感觉是第5节的意思用在于更新参数上。  