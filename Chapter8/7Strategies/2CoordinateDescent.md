在某些情况下，**将一个优化问题分解成几个部分，可以更快地解决原问题**。
如果我们相对于某个单一变量$x_i$最小化$f(x)$，然后相对于另一个变量$x_j$等等，反复循环所有的变量，我们会保证到达（局部）极小值。
这种做法被称为坐标下降，因为我们一次优化一个坐标。
更一般地，块坐标下降是指对于某个子集的变量同时最小化。
术语"坐标下降"通常既指块坐标下降，也指严格的单个坐标下降。

当优化问题中的不同变量能够清楚地分成相对独立的组，或是当优化一组变量明显比优化所有变量效率更高时，坐标下降最有意义。  
> **[success]**  
适用场景：  
（1）参数容易分组   
（2）优化一组变量比优化所有变量效率高  

例如，考虑代价函数
$$
\begin{aligned}
    J(H, W) = \sum_{i,j} |H_{i,j}| + \sum_{i,j} \left( X - W^\top H \right)_{i,j}^2 .
\end{aligned}
$$

该函数描述了一种被称为稀疏编码的学习问题，其目标是寻求一个权重矩阵$W$，可以线性解码激活值矩阵$H$以重构训练集$X$。  
> **[warning]** 稀疏编码的学习问题？  

稀疏编码的大多数应用还涉及到权重衰减或$W$列范数的约束，以避免极小$H$和极大$W$的病态解。

函数$J$不是凸的。
然而，我们可以将训练算法的输入分成两个集合：字典参数$W$和编码表示$H$。
最小化关于这两者之一的任意一组变量的目标函数都是凸问题。
因此，块坐标下降允许我们使用高效的凸优化算法，交替固定$H$优化$W$和固定$W$优化$H$。

当一个变量的值很大程度地影响另一个变量的最优值时，坐标下降不是一个很好的方法，如函数$f(x)=(x_1 - x_2)^2+\alpha(x_1^2 + x_2^2)$，其中$\alpha$是正值常数。  
> **[success]**  
不适用场景：一组变量的变化很大程度地影响另一组变量的最优值。  

第一项鼓励两个变量具有相似的值，而第二项鼓励它们接近零。
解是两者都为零。
牛顿法可以一步解决这个问题，因为它是一个正定二次问题。
但是，对于小值$\alpha$而言，坐标下降会使进展非常缓慢，因为第一项不允许单个变量变为和其他变量当前值显著不同的值。