批标准化~{cite?}是优化深度神经网络中最激动人心的最新创新之一。
实际上它并不是一个优化算法，而是**一个自适应的重参数化的方法**，试图解决训练**非常深的模型**的困难。

非常深的模型会涉及多个函数或层组合。  
> **[success]** 非常深的模型的问题  
w更新算法是在假设其它层w不变的情况下计算某一层的$\Delta w$的。  
但实际上所有w会同时更新。  
相邻层之间w的相互作用对最终结果有比较大的影响。  

在其他层不改变的假设下，梯度用于如何更新每一个参数。
在实践中，我们同时更新所有层。
当我们进行更新时，可能会发生一些意想不到的结果，这是因为许多组合在一起的函数同时改变时，计算更新的假设是其他函数保持不变。
举一个简单的例子，假设我们有一个深度神经网络，每一层只有一个单元，并且在每个隐藏层不使用激活函数：$\hat{y} = xw_1 w_2 w_3 \dots w_l$。
此处，$w_i$表示用于层$i$的权重。层$i$的输出是$h_i = h_{i-1} w_i$。
输出$\hat{y}$是输入$x$的线性函数，但是权重$w_i$的非线性函数。
假设我们的代价函数 $\hat{y}$上的梯度为$1$，所以我们希望稍稍降低$\hat{y}$。
然后反向传播算法可以计算梯度$g = \nabla_{w} \hat{y}$。
想想我们在更新$w \leftarrow w - \epsilon g$时会发生什么。
近似$\hat{y}$的一阶泰勒级数会预测$\hat{y}$的值下降$\epsilon g^\top g$。
如果我们希望$\hat{y}$下降$0.1$，那么梯度中的一阶信息表明我们应设置学习率$\epsilon$为$\frac{0.1}{g^\top g}$。
然而，实际的更新将包括二阶，三阶，直到$l$阶的影响。
$\hat{y}$的更新值为  
$$
\begin{aligned}
    x(w_1-\epsilon g_1)(w_2-\epsilon g_2)\dots(w_l-\epsilon g_l),
\end{aligned}
$$

这个更新中所产生的一个二阶项示例是$\epsilon^2 g_1 g_2 \prod_{i=3}^l w_i$ 。
如果 $\prod_{i=3}^l w_i$很小，那么该项可以忽略不计。而如果层$3$到层$l$的权重都比$1$大时，该项可能会指数级大。
这使得我们很难选择一个合适的学习率，因为某一层中参数更新的效果很大程度上取决于其他所有层。
二阶优化算法通过考虑二阶相互影响来解决这个问题，但我们可以看到，在非常深的网络中，更高阶的相互影响会很显著。
即使是二阶优化算法，计算代价也很高，并且通常需要大量近似，以免真正计算所有的重要二阶相互作用。
因此对于$n>2$的情况，建立$n$阶优化算法似乎是无望的。
那么我们可以做些什么呢？

<!-- % -- 309 -- -->

批标准化提出了一种几乎可以重参数化所有深度网络的优雅方法。
重参数化显著减少了多层之间协调更新的问题。
批标准化可应用于网络的任何输入层或隐藏层。
设$H$是需要标准化的某层的小批量激活函数，排布为设计矩阵，每个样本的激活出现在矩阵的每一行中。  
> **[success]**  
$H_{ij}$为第i个样本在某层第j个unit上的激活函数  

为了标准化$H$，我们将其替换为
$$
\begin{aligned}
H' = \frac{H - \mu}{\sigma},
\end{aligned}
$$

其中$\mu$是包含每个单元均值的向量，$\sigma$是包含每个单元标准差的向量。
此处的算术是基于广播向量$\mu$和向量$\sigma$应用于矩阵$H$的每一行。  
> **[warning]**  
H的每个元素都是函数，函数怎么会有均值和标准差？  

在每一行内，运算是逐元素的，因此$H_{i,j}$标准化为减去$\mu_j$再除以$\sigma_j$。  
> **[success]**  
$$
H'_{ij} = \frac{H_{ij}-\mu_j}{\sigma_j}
$$

网络的其余部分操作$H'$的方式和原网络操作$H$的方式一样。

在训练阶段，  
$$
\begin{aligned}
    \mu = \frac{1}{m} \sum_i H_{i,:}
\end{aligned}
$$

和  
$$
\begin{aligned}
    \sigma = \sqrt{ \delta + \frac{1}{m} \sum_i (H - mu)_i^2 },
\end{aligned}
$$

> **[success]**  
$\delta$用于防止分母为0  

其中$\delta$是个很小的正值，比如$10^{-8}$，以强制避免遇到$\sqrt{z}$的梯度在$z=0$处未定义的问题。
至关重要的是，\emph{我们反向传播这些操作}，来计算均值和标准差，并应用它们于标准化$H$。
这意味着，**梯度不会再简单地增加$h_i$的标准差或均值；标准化操作会除掉这一操作的影响，归零其在梯度中的元素。**  
> **[warning]** ?  

这是批标准化方法的一个重大创新。
以前的方法添加代价函数的惩罚，以鼓励单元标准化激活统计量，或是在每个梯度下降步骤之后重新标准化单元统计量。
前者通常会导致不完全的标准化，而后者通常会显著地消耗时间，因为学习算法会反复改变均值和方差而标准化步骤会反复抵消这种变化。  
> **[warning]** 后者和现在的方法有什么区别？  

批标准化重参数化模型，以使一些单元在定义上就总是标准化的，巧妙地回避了这两个问题。

在测试阶段，$\mu$和$\sigma$可以被替换为训练阶段收集的运行均值。
这使得模型可以对单一样本评估，而无需使用定义于整个小批量的$\mu$和$\sigma$。

> **[warning]** 关于这个例子的解释没看懂？  

回顾例子$\hat{y} = x w_1 w_2 \dots w_l$，我们看到，我们可以通过标准化$h_{l-1}$很大程度地解决了学习这个模型的问题。
假设$x$采样自一个单位高斯分布，
那么$h_{l-1}$也是来自高斯分布，因为从$x$到$h_l$的变换是线性的。
然而，$h_{l-1}$不再有零均值和单位方差。
使用批标准化后，我们得到的归一化$\hat{h}_{l-1}$恢复了零均值和单位方差的特性。
对于底层的几乎任意更新而言，$\hat{h}_{l-1}$仍然保持着单位高斯分布。
然后输出$\hat{y}$可以学习为一个简单的线性函数$\hat{y} = w_l \hat{h}_{l-1}$。
现在学习这个模型非常简单，因为低层的参数在大多数情况下没有什么影响；它们的输出总是重新标准化为单位高斯分布。
只在少数个例中，低层会有影响。
改变某个低层权重为$0$，可能使输出退化；改变低层权重的符号可能反转$\hat{h}_{l-1}$和$y$之间的关系。
这些情况都是非常罕见的。
没有标准化，几乎每一个更新都会对$h_{l-1}$的统计量有着极端的影响。
因此，批标准化显著地使得模型更易学习。
在这个示例中，容易学习的代价是使得底层网络没有用。  
> **[warning]** 使得底层网络没有用?  

在我们的线性示例中，较低层不再有任何有害的影响，但它们也不再有任何有益的影响。
这是因为我们已经标准化了一阶和二阶统计量，这是线性网络可以影响的所有因素。
在具有非线性激活函数的深度神经网络中，较低层可以进行数据的非线性变换，所以它们仍然是有用的。
批标准化仅标准化每个单元的均值和方差，以稳定化学习，但允许单元和单元之间的关系，以及单个单元的非线性统计量之间的关系发生变化。


由于网络的最后一层能够学习线性变换，实际上我们可能希望移除一层内单元之间的所有线性关系。  
> **[warning]** 移除一层内单元之间的所有线性关系?  

> **[warning]** 后面全部没看懂？  

事实上，这是~{Desjardins2015}中采用的方法，为批标准化提供了灵感。
令人遗憾的是，消除所有的线性关联比标准化各个独立单元的均值和标准差代价更高，因此批标准化仍是迄今最实用的方法。

标准化一个单元的均值和标准差会降低包含该单元的神经网络的表达能力。
为了保持网络的表现力，通常会将批量隐藏单元激活$H$替换为$\gamma H' + \beta$，而不是简单地使用标准化的$H'$。
变量$\gamma$和$\beta$是允许新变量有任意均值和标准差的学习参数。
乍一看，这似乎是无用的——为什么我们将均值设为$0$，然后又引入参数允许它被重设为任意值$\beta$？
答案是新的参数可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。
在旧参数中，$H$的均值取决于$H$下层中参数的复杂关联。
在新参数中，$\gamma H' + \beta$的均值仅由$\beta$确定。
新参数很容易通过梯度下降来学习。

大多数神经网络层会采取$\phi(XW+ b)$的形式，其中$\phi$是某个固定的非线性激活函数，如整流线性变换。
自然想到我们应该将批标准化应用于输入$X$还是变换后的值$XW+b$。
{Ioffe+Szegedy-2015}推荐后者。
更具体地，$XW+b$应替换为$XW$的标准化形式。
偏置项应被忽略，因为参数$\beta$会加入批标准化重参数化，它是冗余的。
一层的输入通常是前一层的非线性激活函数（如整流线性函数）的输出。
因此，输入的统计量更符合非高斯分布，而更不服从线性操作的标准化。

\chap?所述的卷积网络，在特征映射中每个空间位置同样地标准化$\mu$和$\sigma$是很重要的，能使特征映射的统计量在不同的空间位置，仍然保持相同。