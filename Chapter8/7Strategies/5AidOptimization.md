改进优化的最好方法并不总是改进优化算法。
相反，深度模型中优化的许多改进来自于设计易于优化的模型。

原则上，我们可以使用呈锯齿非单调模式上上下下的激活函数，但是，这将使优化极为困难。
在实践中，\emph{**选择一族容易优化的模型比使用一个强大的优化算法更重要**}。
神经网络学习在过去30年的大多数进步主要来自于改变模型族，而非改变优化过程。
1980年代用于训练神经网络的带动量的随机梯度下降，仍然是现代神经网络应用中的前沿算法。

> **[success]** 怎样的模型易于优化？  
（1）多使用线性函数  
（2）Jacobian具有合理的值  
（3）线性函数在一个方向上一致地增加  
（4）局部梯度信息合理  
（5）层之间的跳跃连接  
（6）添加“辅助头”  

具体来说，现代神经网络的\emph{设计选择}体现在层之间的线性变换，几乎处处可导的激活函数，和大部分定义域都有明显的梯度。
特别地，创新的模型，如\,LSTM，整流线性单元和\,maxout\,单元都比先前的模型（如基于\,sigmoid\,单元的深度网络）使用更多的线性函数。
这些模型都具有简化优化的性质。
如果线性变换的\,Jacobian\,具有相对合理的奇异值，那么梯度能够流经很多层。
此外，线性函数在一个方向上一致增加，所以即使模型的输出远离正确值，也可以简单清晰地计算梯度，使其输出方向朝降低损失函数的方向移动。
换言之，现代神经网络的设计方案旨在使其\emph{局部}梯度信息合理地对应着移向一个遥远的解。

其他的模型设计策略有助于使优化更简单。
例如，层之间的线性路径或是跳跃连接减少了从较低层参数到输出最短路径的长度，因而缓解了梯度消失的问题{cite?}。
一个和跳跃连接相关的想法是添加和网络中间隐藏层相连的输出的额外副本，如GoogLeNet~{cite?}和深度监督网络{cite?}。
这些"辅助头"被训练来执行和网络顶层主要输出相同的任务，以确保底层网络能够接受较大的梯度。
当训练完成时，辅助头可能被丢弃。
这是之前小节介绍到的预训练策略的替代方法。
以这种方式，我们可以在一个阶段联合训练所有层，而不改变架构，使得中间层（特别是低层）能够通过更短的路径得到一些如何更新的有用信息。
这些信息为底层提供了误差信号。