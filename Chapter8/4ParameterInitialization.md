初始化对于深度学习很重要，因为它能决定：  
1. 是否收敛  
2. 是否稳定  
3. 收敛到哪个点  

改进初始化参数的困难：  
1. 在初始化化时的好性质如何保持下去  
2. 看似对训练有利的初始化实际上对泛化不利  

**初始参数需要在不同单元间“破坏对称性”。**  

# w的初始化
1. 输出和输入一样多，可以使用Gram-Schmidt正交化于初始的权重矩阵。  
2. 在高维空间上使用高熵分布来随机初始化。  
3. 为高斯或均匀分布中随机抽取的值。  
4. 更大的初始权重具有更强的破坏对称性的作用。  
5. 正则化的角度希望使用较小的参数。  
6. 标准初始化  
$$
W_{i,j} \sim U(-\sqrt\frac{6}{m+n}, \sqrt\frac{6}{m+n})
$$
其中，m为输入个数，n为输出个数。  
7. 初始化为随机正交矩阵  
8. 将每层权重的初始数值范围设为超参数，使超参数搜索算法挑选这些数值范围。  

# b的初始化

1. 使用启发式挑选的初值  
2. 必须和设置权重的方法协调。  
3. 设置为0在大多数方案中是可行的。  
4. 输出单元的b应正确设置。  
5. 对于一些特殊的unit，合理设置b以避免unit饱和。  
6. 一个单元通过b控制其他单元能否参与到等式中。

# 方差或精确度参数