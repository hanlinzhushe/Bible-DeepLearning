当计算图变得极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题——**由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难**。 
深层的计算图不仅存在于前馈网络，还存在于之后介绍的循环网络中（在\chap?中描述）。
因为循环网络要在很长时间序列的各个时刻重复应用相同操作来构建非常深的计算图，并且模型参数共享，这使问题更加凸显。

例如，假设某个计算图中包含一条反复与矩阵$W$相乘的路径。
那么$t$步后，相当于乘以$W^t$。
假设$W$有[特征值分解](https://windmissing.github.io/mathematics_basic_for_ML/LinearAlgebra/eigendecomposition.html)$W = V \text{diag}(\lambda) V^{-1}$。
在这种简单的情况下，很容易看出  
$$
\begin{aligned}
  W^t = (V \text{diag}(\lambda) V^{-1})^t = V\text{diag}(\lambda)^t  V^{-1}.
\end{aligned}
$$

当特征值$\lambda_i$的绝对值不在$1$附近时，若绝对值大于$1$则会爆炸；若小于$1$时则会消失。
梯度消失与爆炸问题是指该计算图上的梯度也会因为$\text{diag}(\lambda)^t$大幅度变化。
**梯度消失使得我们难以知道参数朝哪个方向移动能够改进代价函数，而梯度爆炸会使得学习不稳定。**
之前描述的促使我们使用梯度截断的悬崖结构便是梯度爆炸现象的一个例子。

此处描述的在各时间步重复与$W$相乘非常类似于寻求矩阵$W$的最大特征值及对应特征向量的幂方法。
从这个观点来看，$x^\top W^t$最终会丢弃$x$中所有与$W$的主特征向量正交的成分。

循环网络在各时间步上使用相同的矩阵$W$，而前馈网络并没有。
所以即使使用非常深层的前馈网络，也能很大程度上有效地避免梯度消失与爆炸问题{cite?}。

在更详细地描述循环网络之后，我们将会在第10.7进一步讨论循环网络训练中的挑战。