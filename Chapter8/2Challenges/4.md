多层神经网络经常存在像悬崖一样的斜率较大区域，如\fig?所示。
这是由于几个较大的权重相乘导致的。
遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会完全跳过这类悬崖结构。  

> **[success]**  
> **问：什么是悬崖？**  
> 答：斜率较大的区域，在loss function上表现为悬崖。  
> **问：为什么会出现悬崖？**  
> 答：较大权重的相乘  
> **问：悬崖有什么问题？**  
> 答：很大程度地改变参数，参数飞到一个非常遥远的未知位置。  
> **问：解决方法？**  
> 答：梯度截断，10.11.1

{% reveal %} 
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter8/figures/cliff_color}}
\fi
\caption{高度非线性的深度神经网络或循环神经网络的目标函数通常包含由几个参数连乘而导致的参数空间中尖锐非线性。
这些非线性在某些区域会产生非常大的导数。
当参数接近这样的悬崖区域时，梯度下降更新可以使参数弹射得非常远，可能会使大量已完成的优化工作成为无用功。
图经 {Pascanu+al-ICML2013-small}许可改编。}
\end{figure}
{% endreveal %} 

不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是我们可以用使用\sec?介绍的启发式梯度截断来避免其严重的后果。
其基本想法源自梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。
当传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。
悬崖结构在循环神经网络的代价函数中很常见，因为这类模型会涉及到多个因子的相乘，其中每个因子对应一个时间步。
因此，长期时间序列会产生大量相乘。