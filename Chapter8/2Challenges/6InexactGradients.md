大多数优化算法的先决条件都是我们知道精确的梯度或是\,Hessian\,矩阵。
在实践中，通常这些量会有噪声，甚至是有偏的估计。
几乎每一个深度学习算法都需要基于采样的估计，至少使用训练样本的小批量来计算梯度。

在其他情况，我们希望最小化的目标函数实际上是难以处理的。
当目标函数不可解时，通常其梯度也是难以处理的。
在这种情况下，我们只能近似梯度。
这些问题主要出现在第三部分中更高级的模型中。
例如，对比散度是用来近似玻尔兹曼机中难以处理的对数似然梯度的一种技术。  
> **[warning]**  对比散度?玻尔兹曼机?

各种神经网络优化算法的设计都考虑到了梯度估计的缺陷。
我们可以选择比真实损失函数更容易估计的代理损失函数来避免这个问题。