# 鞍点
> **[success]**  
[鞍点](https://windmissing.github.io/mathematics_basic_for_ML/Mathematics/derivative.html)  

对于很多高维非凸函数而言，局部极小值（以及极大值）事实上都远少于另一类梯度为零的点：鞍点。
鞍点附近的某些点比鞍点有更大的代价，而其他点则有更小的代价。
在鞍点处，Hessian矩阵同时具有正负特征值。
位于正特征值对应的特征向量方向的点比鞍点有更大的代价，反之，位于负特征值对应的特征向量方向的点有更小的代价。
我们可以将鞍点视为代价函数某个横截面上的局部极小点，同时也可以视为代价函数某个横截面上的局部极大点。
\fig?\,给了一个示例。

## 鞍点 VS 极小值点  

多类随机函数表现出以下性质：低维空间中，局部极小值很普遍。
**在更高维空间中，局部极小值很罕见，而鞍点则很常见。**  

> **[success]**  
鞍点和极小值点都要求点在所有方向上的偏导都为0，即在每个方向上要么是极小值要么是极大值。  
假设n维空间某个点满足此要求，且它在某个方向上为极小值的概率为0.5。     
那么这个点在n维空间中的极小值点的概率为$0.5^n$，为极大值点的概率为$0.5^n$。为鞍点的概率为$1-0.5^n-0.5^n$。  
当n非常大时，这个点是极小值的概率很小，而是鞍点的概率很大。  

对于这类函数$f:\Bbb R^n \to \Bbb R$而言，鞍点和局部极小值的数目比率的期望随$n$指数级增长。
我们可以从直觉上理解这种现象——Hessian\,矩阵在局部极小点处只有正特征值。
而在鞍点处，Hessian\,矩阵则同时具有正负特征值。
试想一下，每个特征值的正负号由抛硬币决定。
在一维情况下，很容易抛硬币得到正面朝上一次而获取局部极小点。
在$n$-维空间中，要抛掷$n$次硬币都正面朝上的难度是指数级的。 
具体可以参考~{Dauphin-et-al-NIPS2014-small}，它回顾了相关的理论工作。

很多随机函数一个惊人性质是，当我们到达代价较低的区间时，Hessian\,矩阵的特征值为正的可能性更大。
和抛硬币类比，这意味着如果我们处于低代价的临界点时，抛掷硬币正面朝上$n$次的概率更大。
这也意味着，**局部极小值具有低代价的可能性比高代价要大得多。**
**具有高代价的临界点更有可能是鞍点。**
具有极高代价的临界点就很可能是局部极大值了。

以上现象出现在许多种类的随机函数中。
那么是否在神经网络中也有发生呢？
{Baldi89}从理论上证明，不具非线性的浅层自编码器（\chap?中将介绍的一种将输出训练为输入拷贝的前馈网络）只有全局极小值和鞍点，没有代价比全局极小值更大的局部极小值。
他们还发现这些结果能够扩展到不具非线性的更深的网络上，不过没有证明。
这类网络的输出是其输入的线性函数，但它们仍然有助于分析非线性神经网络模型，因为它们的损失函数是关于参数的非凸函数。
这类网络本质上是多个矩阵组合在一起。
{Saxe-et-al-ICLR13}精确解析了这类网络中完整的学习动态，表明这些模型的学习能够捕捉到许多在训练具有非线性激活函数的深度模型时观察到的定性特征。
{Dauphin-et-al-NIPS2014-small}通过实验表明，真实的神经网络也存在包含很多高代价鞍点的损失函数。
{Choromanska-et-al-AISTATS2015}提供了额外的理论论点，表明另一类和神经网络相关的高维随机函数也满足这种情况。

## 鞍点的影响
鞍点激增对于训练算法来说有哪些影响呢？
对于只使用梯度信息的一阶优化算法而言，目前情况还不清楚。
鞍点附近的梯度通常会非常小。
另一方面，实验中梯度下降似乎可以在许多情况下逃离鞍点。
{GoodfellowOptimization15}可视化了最新神经网络的几个学习轨迹，\fig?\,给了一个例子。
这些可视化显示，在突出的鞍点附近，代价函数都是平坦的，权重都为零。
但是他们也展示了梯度下降轨迹能够迅速逸出该区间。
{GoodfellowOptimization15}也主张，应该可以通过分析来表明连续时间的梯度下降会逃离而不是吸引到鞍点，但对梯度下降更现实的使用场景来说，情况或许会有所不同。

{% reveal %}
{% raw %}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter8/figures/plot_atmu_relu_5}}
\fi
\caption{神经网络代价函数的可视化。
这些可视化对应用于真实对象识别和自然语言处理任务的前馈神经网络、卷积网络和循环网络而言是类似的。
令人惊讶的是，这些可视化通常不会显示出很多明显的障碍。
大约2012年，在随机梯度下降开始成功训练非常大的模型之前，相比这些投影所显示的神经网络代价函数的表面通常被认为有更多的非凸结构。
该投影所显示的主要障碍是初始参数附近的高代价鞍点，但如由蓝色路径所示，SGD\,训练轨迹能轻易地逃脱该鞍点。
大多数训练时间花费在横穿代价函数中相对平坦的峡谷，可能由于梯度中的高噪声、或该区域中~Hessian~矩阵的病态条件，或者需要经过间接的弧路径绕过图中可见的高"山" 。
图经~{GoodfellowOptimization15}许可改编。
}
\end{figure}
{% endraw %}
{% endreveal%}

> **[success]**  
如果目标是“寻找梯度为0的点”，会被吸引到鞍点或极大值点。  
例如：[牛顿法](https://windmissing.github.io/mathematics_basic_for_ML/NumericalComputation/Newton.html)  

对于牛顿法而言，鞍点显然是一个问题。
梯度下降旨在朝"下坡"移动，而非明确寻求临界点。
而牛顿法的目标是寻求梯度为零的点。
如果没有适当的修改，牛顿法就会跳进一个鞍点。
高维空间中鞍点的激增或许解释了在神经网络训练中为什么二阶方法无法成功取代梯度下降。
{Dauphin-et-al-NIPS2014-small}介绍了二阶优化的无鞍牛顿法，并表明和传统版本的牛顿法相比有显著改进。
二阶方法目前仍然难以扩展到大型神经网络，但是如果这类无鞍算法能够扩展的话，还是很有希望的。

除了极小值和鞍点，还存在其他梯度为零的点。
例如极大值，它的情况从优化的角度看与鞍点很类似，很多算法不会被吸引到极大值，除了未经修改的牛顿法。
和极小值一样，许多种类的随机函数的极大值在高维空间中也是指数级稀少。

# 平坦区域

也可能存在恒值的、宽且平坦的区域。
在这些区域，梯度和\,Hessian\,矩阵都是零。
这种退化的情形对所有数值优化算法而言都是主要问题。
在凸问题中，一个宽而平坦的区间肯定包含全局极小值，但是对于一般的优化问题而言，
这样的区域可能会对应着目标函数中一个较高的值。
> **[success]**  
Momentum、RMSProp、Adam等算法能够加速平坦区域的学习。  

