在本节中，我们讨论了一系列算法，通过自适应每个模型参数的学习率以解决优化深度模型中的难题。
此时，一个自然的问题是：该选择哪种算法呢？

遗憾的是，目前在这一点上没有达成共识。
{Schaul2014_unittests}展示了许多优化算法在大量学习任务上极具价值的比较。
虽然结果表明，具有自适应学习率（以RMSProp和AdaDelta为代表）的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 
> **[warning]** AdaDelta  

目前，最流行并且使用很高的优化算法包括SGD、具动量的SGD、RMSProp、具动量的RMSProp、AdaDelta和Adam。
此时，选择哪一个算法似乎主要取决于使用者对算法的熟悉程度（以便调节超参数）。  

> **[success]**  
$\Delta\theta = -\eta g$
第三节是关于怎么更新g的算法  
第五节是关于怎么更新$\eta$的算法  
两种算法可以结合使用