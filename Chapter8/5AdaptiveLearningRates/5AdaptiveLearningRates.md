# 8.5.1 AdaGrad

独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。  

优点：  
1. 具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。  
2. 在参数空间中更为平缓的倾斜方向会取得更大的进步。  
缺点：  
从训练开始时积累梯度平方会导致有
效学习率过早和过量的减小。

# 8.5.2 RMSProp

daGrad算法的改进，改变梯度积累为指数加权的移动平均。  

**有效且实用**

# 8.5.3 Adam

结合RMSProp 和具有一些重要区别的动量的变种。  
1. 将动量应用于缩放后的梯度。  
2. 偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。  

**对超参数的选择相当鲁棒**
