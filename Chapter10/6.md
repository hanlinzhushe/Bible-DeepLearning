递归神经网络\footnote{我们建议不要将"递归神经网络"缩写为"RNN"，以免与"循环神经网络"混淆。}代表循环网络的另一个扩展，它被构造为**深的树状结构而不是RNN的链状结构**，因此是不同类型的计算图。  
> **[success]** 递归神经网络不是一种扎，因为它不是链状结构  

递归网络的典型计算图如\fig?所示。
递归神经网络由{Pollack90}引入，而{tr-bottou-2011}描述了这类网络的**潜在用途——学习推论**。
递归网络已成功地**应用于输入是\emph{数据结构}的神经网络**{cite?}，如自然语言处理{cite?}和计算机视觉{cite?}。

递归网络的一个明显优势是，对于具有相同长度$\tau$的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从$\tau$减小为$\Bbb O(\log \tau)$，这可能有助于解决长期依赖。  
> **[success]**  
> 优点：深度减少，有助于解决长期依赖问题。  

一个悬而未决的问题是**如何以最佳的方式构造树**。
一种选择是使用不依赖于数据的树结构，如平衡二叉树。
在某些应用领域，外部方法可以为选择适当的树结构提供借鉴。
例如，处理自然语言的句子时，用于递归网络的树结构可以被固定为句子语法分析树的结构（可以由自然语言语法分析程序提供）{cite?}。
理想的情况下，人们希望学习器自行发现和推断适合于任意给定输入的树结构，如{cite?}所建议。

{% reveal %}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/recursive_net}}
\fi
\caption{递归网络将循环网络的链状计算图推广到树状计算图。
可变大小的序列$x^{(1)},x^{(2)},\dots,x^{(t)}$可以通过固定的参数集合（权重矩阵$U,V,W$）映射到固定大小的表示（输出$o$）。
该图展示了监督学习的情况，其中提供了一些与整个序列相关的目标$y$。
}
\end{figure}
{% endreveal %}

递归网络想法的变种存在很多可能。
例如，{Frasconi97}和{Frasconi-1998}将数据与树结构相关联，并将输入和目标与树的单独节点相关联。
由每个节点执行的计算无须是传统的人工神经计算（所有输入的仿射变换后跟一个单调非线性）。
例如，{Socher-et-al-EMNLP2013}提出用张量运算和双线性形式，在这之前人们已经发现当概念是由连续向量（嵌入）表示时，这种方式有利于建模概念之间的联系{cite?}。  
> **[warning]** 最后一段没看懂。  

