仅在一个时间步的输出和下一个时间步的隐藏单元间存在循环连接的网络（示于\fig?）确实没有那么强大（因为缺乏隐藏到隐藏的循环连接）。
例如，它不能模拟通用图灵机。
因为这个网络缺少隐藏到隐藏的循环，它要求输出单元捕捉用于预测未来的关于过去的所有信息。
因为输出单元明确地训练成匹配训练集的目标，它们不太能捕获关于过去输入历史的必要信息，除非用户知道如何描述系统的全部状态，并将它作为训练目标的一部分。
消除隐藏到隐藏循环的优点在于，任何基于比较时刻$t$的预测和时刻$t$的训练目标的损失函数中的所有时间步都解耦了。
因此训练可以并行化，即在各时刻$t$分别计算梯度。
没有必要先计算前一时刻的输出，因为训练集提供了前一时刻输出的理想值。  
> **[success]** 这一段在比较图10.3和图10.4这两种网络的优缺点：  
图10.3是“有隐藏到隐藏的循环连接（h->h）”的RNN架构。  
它的优点是有t-1时刻的h作为t时刻的输入，而t-1时刻的h能够很好地包含“用于预测未来的关于过去的所有信息”，所以更加强大（能模拟通用图灵机）。[?]但我不明白为什么作者一定要把RNN和图灵机比较。  
与之对比的(y->h)使用t-1的y作为输入,t-1的y是努力拟合设计者要求它成为的样子，而不是“捕捉过去的信息”。  
图10.4是“消除隐藏到隐藏循环（y->h）”的架构。  
它的优点是可以通过“导师驱动”方法（下文会介绍这种方法）使得t时刻与t-1时刻能够同时计算，包括正向求值和反向求导都能在所有时刻实现并行计算。  
而h->h无法做到并行计算，必须先计算出t-1的h，才能计算t。  

由输出反馈到模型而产生循环连接的模型可用**导师驱动过程**（teacher forcing）进行训练。
导师驱动过程是从最大似然准则派生出来的一个训练过程，训练模型时，在时刻$t+1$接收真实值$y^{(t)}$作为输入。  
> **[success] 时间步解耦**  
令$\hat y$为某个时刻的预测输出，y为某个时间的期望输出。  
$\hat y$是要在特定时刻算出来的，y是一开始就知道的。  
t时刻的计算要依赖t-1时刻的$\hat y$。因此要t-1时刻算完了得到$\hat y$了才能做t时刻的计算。  
已知$\hat y$的目标是模拟y，所以直接用y代替$\hat y$作为t时刻的输入。  
y是一开始就知道的，不需要等到t-1时刻的计算。  
因此t时刻与t-1时刻能够并行计算。  

我们可以通过检查两个时间步的序列看清楚这一点。
条件最大似然准则是  
> **[success]**  
最大似然原则，是要选择最合适的$y^{(2)}$，使得公式10.15达到最大。   
在这里$y^{(1)}$、$y^{(2)}$是预测值，即上文中的$\hat y$  
但又为了并行计算，把$\hat y^{(2)}$又替换成了真正的$y^{(2)}$  

$$
\begin{aligned}
 &\log p(y^{(1)},y^{(2)} \mid x^{(1)}, x^{(2)} ) \\
 &= \log  p(y^{(2)} \mid y^{(1)}, x^{(1)}, x^{(2)} )  + \log p(y^{(1)} \mid x^{(1)}, x^{(2)}) && (10.15)
\end{aligned}
$$

> **[warning]**  
这里的y是对应上一节非归一化的对数概率o还是归一化的对数概率$\hat y$？  
从名字上看应该是后者。  
但在后面的图上，计算L时又用的是o。  

在这个例子中，\emph{同时}给定迄今为止的$x$序列和来自训练集的前一$y$值，我们可以看到在时刻$t=2$时，模型被训练为最大化$y^{(2)}$的条件概率。
因此最大似然在训练时指定使用正确的目标值作为反馈，而不是将自己的输出反馈到模型。
如\fig?所示。  

{% reveal %}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/teacher_forcing}}
\fi
\caption{导师驱动过程的示意图。
导师驱动过程是一种训练技术，适用于输出与下一时间步的隐藏状态存在连接的RNN。
\emph{(左)}训练时，我们将训练集中\emph{正确}的输出$y^{(t)}$反馈到$h^{(t+1)}$。
\emph{(右)}当模型部署后，真正的输出通常是未知的。
在这种情况下，我们用模型的输出$o^{(t)}$近似正确的输出$y^{(t)}$，并反馈回模型。
}
\end{figure}
{% endreveal %}

我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。  
> **[warning]** [?]在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播  

对于那些存在隐藏到隐藏连接的模型，只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以适用。
只不过，只要隐藏单元成为较早时间步的函数，BPTT算法就是必要的。
因此训练某些模型时可能要同时使用导师驱动过程和BPTT。  
> **[success]**  
模型中存在y->h，就可以使用导师驱动过程。  
模型中存在?->h（?代表t-1时刻的某个输出，而且这个输出只能等t-1时刻算完了才知道，这种场景无法并行计算），就需要使用BPTT。  
模型中同时存在y->h和？->h，就会同时使用导师驱动过程和BPTT。  

如果之后网络在**开环**(open-loop)模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。  
> **[warning]**  
训练时使用y->h，预测时使用$\hat y$ -> h，这样会出什么问题？  

在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。
减轻此问题的一种方法是同时使用导师驱动过程和自由运行的输入进行训练，例如在展开循环的输出到输入路径上预测若干个步骤之后的正确目标值。  
> **[success]**  
开环模型：测试时$\hat y$ -> h  
导师驱动：训练时y->h  
自由运行：训练时$\hat y$ -> h  
为了解决“训练期间该网络看到的输入与测试时看到的会有很大的不同”的问题，作者建议结合使用导师驱动和自由运行。即结合使用y和$\hat y$。    

　　
> **[warning]** 结合方式1，没看懂。  

通过这种方式，网络可以学会考虑在训练时没有接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态映射回使网络几步之后生成正确输出的状态。
另外一种方式{cite?}是通过随机选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。
这种方法利用了课程学习策略，逐步使用更多生成值作为输入。
> **[success]**  
结合方式2，随机的使用y和$\hat y$  