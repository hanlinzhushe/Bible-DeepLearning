仅在一个时间步的输出和下一个时间步的隐藏单元间存在循环连接的网络（示于\fig?）确实没有那么强大（因为缺乏隐藏到隐藏的循环连接）。
例如，它不能模拟通用图灵机。
因为这个网络缺少隐藏到隐藏的循环，它要求输出单元捕捉用于预测未来的关于过去的所有信息。
因为输出单元明确地训练成匹配训练集的目标，它们不太能捕获关于过去输入历史的必要信息，除非用户知道如何描述系统的全部状态，并将它作为训练目标的一部分。
消除隐藏到隐藏循环的优点在于，任何基于比较时刻$t$的预测和时刻$t$的训练目标的损失函数中的所有时间步都解耦了。
因此训练可以并行化，即在各时刻$t$分别计算梯度。
没有必要先计算前一时刻的输出，因为训练集提供了前一时刻输出的理想值。

由输出反馈到模型而产生循环连接的模型可用导师驱动过程进行训练。
导师驱动过程是从最大似然准则派生出来的一个训练过程，训练模型时，在时刻$t+1$接收真实值$y^{(t)}$作为输入。
我们可以通过检查两个时间步的序列看清楚这一点。
条件最大似然准则是
\begin{aligned}
 &\log p(y^{(1)},y^{(2)} \mid x^{(1)}, x^{(2)} ) \\
 &= \log  p(y^{(2)} \mid y^{(1)}, x^{(1)}, x^{(2)} )  + \log p(y^{(1)} \mid x^{(1)}, x^{(2)}) .
\end{aligned}

<!-- % -- 372 -- -->

在这个例子中，\emph{同时}给定迄今为止的$x$序列和来自训练集的前一$y$值，我们可以看到在时刻$t=2$时，模型被训练为最大化$y^{(2)}$的条件概率。
因此最大似然在训练时指定使用正确的目标值作为反馈，而不是将自己的输出反馈到模型。
如\fig?所示。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/teacher_forcing}}
\fi
\caption{导师驱动过程的示意图。
导师驱动过程是一种训练技术，适用于输出与下一时间步的隐藏状态存在连接的~RNN。
\emph{(左)}训练时，我们将训练集中\emph{正确}的输出$y^{(t)}$反馈到$h^{(t+1)}$。
\emph{(右)}当模型部署后，真正的输出通常是未知的。
在这种情况下，我们用模型的输出$o^{(t)}$近似正确的输出$y^{(t)}$，并反馈回模型。
}
\end{figure}

我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。
对于那些存在隐藏到隐藏连接的模型，只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以适用。
只不过，只要隐藏单元成为较早时间步的函数，BPTT~算法就是必要的。
因此训练某些模型时可能要同时使用导师驱动过程和~BPTT。

<!-- % -- 373 -- -->

如果之后网络在\textbf{开环}(open-loop)模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。
在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。
减轻此问题的一种方法是同时使用导师驱动过程和自由运行的输入进行训练，例如在展开循环的输出到输入路径上预测若干个步骤之后的正确目标值。
通过这种方式，网络可以学会考虑在训练时没有接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态映射回使网络几步之后生成正确输出的状态。
另外一种方式{cite?}是通过随机选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。
这种方法利用了课程学习策略，逐步使用更多生成值作为输入。