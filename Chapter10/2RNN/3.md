> **[warning]** “作为有向图模型”是什么意思？  

目前为止，我们接触的循环网络例子中损失$L^{(t)}$是训练目标$y^{(t)}$和输出$o^{(t)}$之间的交叉熵。
> **[success]**  
> 符号$y^{(t)}$用得有点歧义。  
> 有的地方$y^{(t)}$表示为训练目标，一开始就确定的。  
> 有的地方$y^{(t)}$表示为模型在t时刻产生的结果，相当于$\hat y^{{t}}$或$o^{(t)}$。  

与前馈网络类似，原则上循环网络**几乎可以使用任何损失**。
但**必须根据任务来选择损失。**
如前馈网络，我们通常希望将RNN的输出解释为一个概率分布，并且我们通常使用与分布相关联的交叉熵来定义损失。
均方误差是与单位高斯分布的输出相关联的交叉熵损失，例如前馈网络中所使用的。

当我们使用一个预测性对数似然的训练目标，如\eqn?，我们将RNN训练为能够根据之前的输入估计下一个序列元素$y^{(t)}$的条件分布。
这可能意味着，我们最大化对数似然  
$$
\begin{aligned}
 \log p(y^{(t)} \mid x^{(1)},\dots, x^{(t)}),
\end{aligned}
$$

或者，如果模型包括来自一个时间步的输出到下一个时间步的连接，  
$$
\begin{aligned}
 \log p(y^{(t)} \mid x^{(1)},\dots, x^{(t)},y^{(1)},\dots, y^{(t-1)} ).
\end{aligned}
$$

将整个序列$y$的联合分布分解为一系列单步的概率预测是捕获关于整个序列完整联合分布的一种方法。  
> **[success]**  
> 整个序列$y$的联合分布:$p(y1, y2, ..., yt)$  
> 一系列单步的概率:$p(y1),p(y2), ..., p(yt)$  

当我们不把过去的$y$值反馈给下一步作为预测的条件时，那么有向图模型不包含任何从过去$y^{(i)}$到当前$y^{(t)}$的边。   
> **[success]** "不把过去的$y$值反馈给下一步作为预测的条件"  
> 我理解是不存在$\hat y$ -> h，  
> 但仍可以存在y->h或h->h  

在这种情况下，给定x序列后，输出的y值之间条件独立。在这种情况下，输出$y$与给定的$x$序列是条件独立的。  
> **[warning]** [?]输出$y$与给定的$x$序列是条件独立的  

当我们反馈真实的$y$值（不是它们的预测值，而是真正观测到或生成的值）给网络时，那么有向图模型包含所有从过去$y^{(i)}$到当前$y^{(t)}$的边。  
> **[success]**  
> 什么是真实的y?这句话有歧义。  
> 训练目标y和预测结果$\hat y$都是真实的。  
> 预测值和生成的值都是$\hat y$的意思。  
> 这里到底想反馈的是什么？  
> 我理解是$\hat y$  

{% reveal %}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/fully_connected_chain}}
\fi
\caption{序列$y^{(1)},y^{(2)},\dots,y^{(t)},\dots$的全连接图模型。
给定先前的值，每个过去的观察值$y^{(i)}$可以影响一些$y^{(t)}$($t>i$)的条件分布。
当序列中每个元素的输入和参数的数目越来越多，根据此图直接参数化图模型（如\eqn?中）可能是非常低效的。
RNN可以通过高效的参数化获得相同的全连接，如\fig?所示。
}
\end{figure}
{% endreveal %} 

举一个简单的例子，让我们考虑对标量随机变量序列$ \BbbY = \{y^{(1)},\dots,y^{(\tau)}\}$建模的RNN，也没有额外的输入$x$。
在时间步$t$的输入仅仅是时间步$t-1$的输出。
该RNN定义了关于$y$变量的有向图模型。
我们使用链式法则（用于条件概率的\eqn?）参数化这些观察值的联合分布：  
$$
\begin{aligned}
 P(\Bbb Y) = P(\RVy^{(1)},\dots,\RVy^{(\tau)}) = \prod_{t=1}^{\tau}P(\RVy^{(t)} \mid \RVy^{(t-1)},\RVy^{(t-2)},
 \dots,\RVy^{(1)}),
\end{aligned}
$$

其中当$t=1$时竖杠右侧显然为空。
因此，根据这样一个模型，一组值$\{y^{(1)},\dots,y^{(\tau)} \}$的负对数似然为  
$$
\begin{aligned}
 L = \sum_{t} L^{(t)},
\end{aligned}
$$

其中
$$
\begin{aligned}
 L^{(t)} = -\log P(y^{(t)} = y^{(t)} \mid y^{(t-1)},y^{(t-2)}, \dots, y^{(1)}).
\end{aligned}
$$

{% reveal %} 
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/rnn_graphical_model_with_state}}
\fi
\caption{在RNN图模型中引入状态变量，尽管它是输入的确定性函数，但它有助于我们根据\eqn?获得非常高效的参数化。
序列中的每个阶段（对于$h^{(t)}$和$y^{(t)}$）使用相同的结构（每个节点具有相同数量的输入），并且可以与其他阶段共享相同的参数。
}
\end{figure}
{% endreveal %} 

> **[warning]** 从这里开始看不懂了。 

图模型中的边表示哪些变量直接依赖于其他变量。
许多图模型的目标是省略不存在强相互作用的边以实现统计和计算的效率。
例如，我们通常可以作Markov假设，即图模型应该只包含从$\{ y^{(t-k)}, \dots, y^{(t-1)}\}$到$y^{(t)}$的边，而不是包含整个过去历史的边。
然而，在一些情况下，我们认为整个过去的输入会对序列的下一个元素有一定影响。
当我们认为$y^{(t)}$的分布可能取决于遥远过去(在某种程度)的$y^{(i)}$的值，且无法通过$y^{(t-1)}$捕获$y^{(i)}$的影响时，RNN将会很有用。

解释RNN作为图模型的一种方法是将RNN视为定义一个结构为完全图的图模型，且能够表示任何一对$y$值之间的直接联系。
\fig?是关于$y$值且具有完全图结构的图模型。
该RNN完全图的解释基于排除并忽略模型中的隐藏单元$h^{(t)}$。

更有趣的是，将隐藏单元$h^{(t)}$视为随机变量，从而产生RNN的图模型结构\footnote{给定这些变量的父变量，其条件分布是确定性的。
尽管设计具有这样确定性的隐藏单元的图模型是很少见的，但这是完全合理的。}。
在图模型中包括隐藏单元预示RNN能对观测的联合分布提供非常有效的参数化。
假设我们用表格表示法来表示离散值上任意的联合分布，即对每个值可能的赋值分配一个单独条目的数组，该条目表示发生该赋值的概率。
如果$y$可以取$k$个不同的值，表格表示法将有$\Bbb O(k^\tau)$个参数。
对比RNN，由于参数共享，RNN的参数数目为$\Bbb O(1)$且是序列长度的函数。 % ??
我们可以调节RNN的参数数量来控制模型容量，但不用被迫与序列长度成比例。
\eqn?展示了所述RNN通过循环应用相同的函数$f$以及在每个时间步的相同参数$\theta$，有效地参数化的变量之间的长期联系。
\fig?说明了这个图模型的解释。
在图模型中结合$h^{(t)}$节点可以用作过去和未来之间的中间量，从而将它们解耦。
遥远过去的变量$y^{(i)}$可以通过其对$h$的影响来影响变量$y^{(t)}$。
该图的结构表明可以在时间步使用相同的条件概率分布有效地参数化模型，并且当观察到全部变量时，可以高效地评估联合分配给所有变量的概率。


即便使用高效参数化的图模型，某些操作在计算上仍然具有挑战性。
例如，难以预测序列中缺少的值。

循环网络为减少的参数数目付出的代价是\emph{优化}参数可能变得困难。

在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假设。
也就是说，假设给定时刻$t$的变量后，时刻$t +1$变量的条件概率分布是平稳的，这意味着之前的时间步与下个时间步之间的关系并不依赖于$t$。
原则上，可以使用$t$作为每个时间步的额外输入，并让学习器在发现任何时间依赖性的同时，在不同时间步之间尽可能多地共享。
相比在每个$t$使用不同的条件概率分布已经好很多了，但网络将必须在面对新$t$时进行外推。

为了完整描述将RNN作为图模型的观点，我们必须描述如何从模型采样。
我们需要执行的主要操作是简单地从每一时间步的条件分布采样。
然而，还有一个额外的复杂性。
RNN必须有某种机制来确定序列的长度。
这可以通过多种方式实现。

在当输出是从词汇表获取的符号的情况下，我们可以添加一个对应于序列末端的特殊符号{cite?}。
当产生该符号时，采样过程停止。
在训练集中，我们将该符号作为序列的一个额外成员，即紧跟每个训练样本$x^{(\tau)}$之后。

另一种选择是在模型中引入一个额外的Bernoulli输出，表示在每个时间步决定继续生成或停止生成。
相比向词汇表增加一个额外符号，这种方法更通用，因为它适用于任何RNN，而不仅仅是输出符号序列的RNN。
例如，它可以应用于一个产生实数序列的RNN。
新的输出单元通常使用sigmoid单元，并以交叉熵作为损失函数进行训练。
在这种方法中，sigmoid被训练为最大化正确预测的对数似然，即在每个时间步序列决定结束或继续。

确定序列长度$\tau$的另一种方法是将一个额外的输出添加到模型并预测整数$\tau$本身。
模型可以采出$\tau$的值，然后采$\tau$步有价值的数据。
这种方法需要在每个时间步的循环更新中增加一个额外输入，使得循环更新知道它是否是靠近所产生序列的末尾。
这种额外的输入可以是$\tau$的值，也可以是$\tau - t$即剩下时间步的数量。
如果没有这个额外的输入，RNN可能会产生突然结束序列，如一个句子在最终完整前结束。
此方法基于分解  
$$
\begin{aligned}
 P(x^{(1)},\dots, x^{(\tau)}) = P(\tau) P(x^{(1)},\dots,x^{(\tau)} \mid \tau) .
\end{aligned}
$$

直接预测$\tau$的例子见{Goodfellow+et+al-ICLR2014a}。