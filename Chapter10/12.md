智能需要知识并且可以通过学习获取知识，这已促使大型深度架构的发展。
然而，知识是不同的并且种类繁多。
有些知识是隐含的、潜意识的并且难以用语言表达——比如怎么行走或狗与猫的样子有什么不同。
其他知识可以是明确的、可陈述的以及可以相对简单地使用词语表达——每天常识性的知识，如"猫是一种动物"，或者为实现自己当前目标所需知道的非常具体的事实，如"与销售团队会议在141室于下午3:00开始"。


神经网络擅长存储隐性知识，但是他们很难记住事实。
被存储在神经网络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该输入也不会被特别精确地存储。
{Graves-et-al-arxiv2014}推测这是因为神经网络缺乏\textbf{工作存储}(working memory)系统，即类似人类为实现一些目标而明确保存和操作相关信息片段的系统。
这种外显记忆组件将使我们的系统不仅能够快速"故意"地存储和检索具体的事实，也能利用他们循序推论。
神经网络处理序列信息的需要，改变了每个步骤向网络注入输入的方式，长期以来推理能力被认为是重要的，而不是对输入做出自动的、直观的反应{cite?} 。  
> **[success]** 神经网络学习知识的局限性：  
> （1）隐性知识 --- 擅长  
> （2）明确知识 --- 难以记住，不会精确地存储  
> 解决方法：记忆网络  
> （1）一组记忆单元：LSTM、GRU -> 向量  
> （2）监督信号 -> 软件注意机制  

为了解决这一难题，{Weston2014}引入了记忆网络，其中包括一组可以通过寻址机制来访问的记忆单元。
记忆网络原本需要监督信号指示他们如何使用自己的记忆单元。
{Graves-et-al-arxiv2014}引入的神经网络图灵机，不需要明确的监督指示采取哪些行动而能学习从记忆单元读写任意内容，并通过使用基于内容的软注意机制（见{Bahdanau-et-al-ICLR2015-small}和\sec?），允许端到端的训练。  
> **[success] 端到端学习**  
以语音识别为例：  
pipeline学习：  
audio(X) --(MFCC)--> feature --(DL)--> Phonemes ----> words ----> transcript(y)  
end-to-end学习：  
audio(X) ----> transcript(y)  
中间方法：  
audio(X) ----> Phonemes ----> transcript(y)  
**端到端学习的优点与缺点：**  
优点：(1)let the data speak(2)需要手工设计的组件更少  
缺点：(1)需要大量（X,y）数据(2)排除了可能有用的手工设计的组件  
**手工设计的组件**可以把人类知识直接注入算法。它可以对算法起积极作用，也可以起消极作用。  

这种软寻址机制已成为其他允许基于梯度优化的模拟算法机制的相关架构的标准{cite?}。

> **[warning]** 后面只知道大概要干什么，具体原理不懂。  

每个记忆单元可以被认为是LSTM和GRU中记忆单元的扩展。
不同的是，网络输出一个内部状态来选择从哪个单元读取或写入，正如数字计算机读取或写入到特定地址的内存访问。

产生确切整数地址的函数很难优化。
为了缓解这一问题，NTM实际同时从多个记忆单元写入或读取。
读取时，它们采取许多单元的加权平均值。
写入时，他们对多个单元修改不同的数值。
用于这些操作的系数被选择为集中在少数单元，如通过softmax函数产生它们。
使用这些具有非零导数的权重允许控制访问存储器的函数能使用梯度下降法优化。
关于这些系数的梯度指示着其中每个参数是应该增加还是减少，但梯度通常只在接收大系数的存储器地址上具有大的值。

这些记忆单元通常扩充为包含向量，而不是由LSTM或GRU存储单元所存储的单个标量。
增加记忆单元大小的原因有两个。
原因之一是，我们已经增加了访问记忆单元的成本。
我们为产生用于许多单元的系数付出计算成本，但我们预期这些系数聚集在周围小数目的单元。
通过读取向量值，而不是一个标量，我们可以抵消部分成本。 
使用向量值的记忆单元的另一个原因是，它们允许\textbf{基于内容的寻址}(content-based addressing)，其中从一个单元读或写的权重是该单元的函数。
如果我们能够产生一个模式来匹配某些但并非所有元素，向量值单元允许我们检索一个完整向量值的记忆。
这类似于人们能够通过几个歌词回忆起一首歌曲的方式。
我们可以认为基于内容的读取指令是说，"检索一首副歌歌词中带有'我们都住在黄色潜水艇'的歌"。
当我们要检索的对象很大时，基于内容的寻址更为有用——如果歌曲的每一个字母被存储在单独的记忆单元中，我们将无法通过这种方式找到他们。
通过比较，\textbf{基于位置的寻址}(location-based addressing)不允许引用存储器的内容。
我们可以认为基于位置的读取指令是说"检索347档的歌的歌词"。
当存储单元哪怕很小时，基于位置的寻址常常也可以是完全合理的机制。

如果一个存储单元的内容在大多数时间步上会被复制（不被忘记），则它包含的信息可以在时间上向前传播，梯度随时间向后传播，而不会消失或爆炸。

{% reveal %}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter10/figures/memory_network}}
\fi
\caption{具有外显记忆网络的示意图，具备神经网络图灵机的一些关键设计元素。
在此图中，我们将模型的"表示"部分（"任务网络"，这里是底部的循环网络）与存储事实的模型（记忆单元的集合）的"存储器"部分区分开。
任务网络学习"控制"存储器，决定从哪读取以及在哪写入（通过读取和写入机制，由指向读取和写入地址的粗箭头指示）。
}
\end{figure}
{% endreveal %}

外显记忆的方法在\fig?说明，其中我们可以看到一个"任务神经网络"搭配了一个存储器。
虽然这一任务神经网络可以是前馈或循环的，但整个系统是一个循环网络。
任务网络可以选择读取或写入特定的存储器地址。
外显记忆似乎允许模型学习普通RNN或LSTM RNN不能学习的任务。
这种优点的一个原因可能是因为信息和梯度可以（分别在时间上向前和向后）在非常长的持续时间内传播。

作为存储器单元的加权平均值反向传播的替代，我们可以将存储器寻址系数解释为概率，并依据此概率随机从一个单元读取{cite?}。
优化离散决策的模型需要专门的优化算法，这将在\sec?中描述。
目前为止，训练这些做离散决策的随机架构，仍比训练进行软判决的确定性算法更难。

无论是软（允许反向传播）或随机硬性的，用于选择一个地址的机制与先前在机器翻译的背景下引入的注意力机制形式相同{cite?}，这在\sec?中也有讨论。
甚至更早之前，注意力机制的想法就被引入了神经网络，在手写生成的情况下{cite?}，有一个被约束为通过序列只向前移动的注意力机制。
在机器翻译和记忆网络的情况下，每个步骤中关注的焦点可以移动到一个完全不同的地方(相比之前的步骤)。

循环神经网络提供了将深度学习扩展到序列数据的一种方法。
它们是我们的深度学习工具箱中最后一个主要的工具。
现在我们的讨论将转移到如何选择和使用这些工具，以及如何在真实世界的任务中应用这些工具。