# basic function
不管sequence有多长，都会反复不断使用的function  
6'05''

# Naive RNN  
$$
h' = \sigma(W^h h + W^i x)  
y = \sigma(W^o h')
$$

10'45''

# LSTM  
$C^t$ is $C^{t-1}$ added by something ---> C变化慢  
h^t和h^{t-1}可以完全不一样 ---》 h变化快  
步骤：  
1. x^t和h^{t-1}拼在一起，形成新的向量x'  
2.  
$$
z = tanh(w x')  \\
z^i = \sigma(W^i x')  \\
z^f = \sigma(W^f x')  \\
z^o = \sigma(W^o x')  
$$
有时也把c^{t-1}拼到一起形式x',这种情况下C^{t-1}对应的W部分是对角的。  20'51''右上角图  
3.  
$$
C^t = z^f \odot C^{t-1} + z^i\odot z  \\
h^t = z^o \odot tanh(C^t) \\
y^t = \sigma(W'h^t)
$$

24'42''   25'20

# GRU
26'43''  
1. h^{t-1}和x^t合并成一个向量  
2. 分别通过W^r和W^z作transform得到r和z，r是reset gate，z是update gate  
3. h^{t-1}\odot r的结果appy transform到x^t，得到h'  
4. h^t = z\odot h^{t-1} + (1-z)\odot h'
h'和h^{t-1}只有一个对h^t影响大。  
29'00''

# GRU VS LSTM
公式上看，GRU的h^t变化慢，类似LSTM和C^t  
GRU比LSTM少了一组参数。  
$$
C^t = z^f \odot C^{t-1} + z^i\odot z \\
h^t = z\odot h^{t-1} + (1-z)\odot h'
$$

对比发现，z兼任了forget gate和input gate。旧的不去，新的不来的思想。  

# 各种LSTM变种性能的比较 41'36''  
1. std LSTM works well  
2. 将forget gate和input gate联运，参数变少，平均性能没有下降  
3. 去掉peephold，参数量增加，性能没有明显下降  
4. forget gate和output gate对性能很重要。  

# 由RNN产生句子

句子由字符/单词组成，称为一个token  
RNN每次产生一个token  
x: 上一次产生的token  
y: 预测的所有token的分布，需要使用sample或argmax从分布中选一个token  
## 使用
58'31''
y1 = p(W|<BOS>)  <BOS> = begin of sentence  
y2 = p(W|<BOS>, sample(y1))
y3 = p(W|<BOS>, sample(y1), sample(y2))
直至生成token<EOS>

问：为什么公式y2中仍以<BOS>为条件？  
答：<BOS>通过路径h'影响y2  
## 训练
输入x1,<BOS>，计算corss-entropy(y1, 目标1)
输入x2，计算corss-entropy(y2, 目标2)
。。。
总代价为所有单个单价之和。  
59'50''

# 用RNN产生 图片  
把图片看成一个句子
每个pixel是一个token  
1:03'21''

# 条件序列生成 conditional sequence generation

## Image Caption Generation

输入一张图片，输出一句话  
图-> CNN -> 向量  
这个向量所有RNN每个时刻的输入。  
所以时刻都输出这同一个向量。  
t0时刻再输入一个<BOS>  
1:06'59''

## 以字符序列为条件  
例如：翻译，chat-bot  
先用一个向量包含整个句子的信息（RNN encoding）  
把向量作为每个tt的输入(RNN docoder)  
encoder和decoder是一起train的  
又叫seq2seq learning

# 动态条件序列生成 Dynamic Conditional Generation