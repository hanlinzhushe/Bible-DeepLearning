**梯度截断有助于处理爆炸的梯度，但它无助于消失的梯度。**
为了解决消失的梯度问题并更好地捕获长期依赖，我们讨论了如下想法：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近1的部分创建路径。
在\sec?中已经讨论过，实现这一点的一种方法是使用LSTM以及其他自循环和门控机制。  
> **[warning]**  
> "沿着与弧边相关联的梯度乘积接近1的部分创建路径"和"LSTM"是什么关系？  

另一个想法是正则化或约束参数，以引导"信息流"。
特别是即使损失函数只对序列尾部的输出作惩罚，我们也希望梯度向量$\nabla_{h^{(t)}} L$在反向传播时能维持其幅度。  
> **[success]**  
> 解决梯度爆炸问题：梯度截断  
> 解决梯度消失问题：  
> （1）沿着与弧边相关联的梯度乘积接近1的部分创建路径，例如LSTM  
> （2）正则化或约束参数
形式上，我们要使   
> **[warning]** 后面的公式看不懂  

$$
\begin{aligned}
 (\nabla_{h^{(t)}} L) \frac{\partial h^{(t)}}{\partial h^{(t-1)}}
\end{aligned}
$$

与  
$$
\begin{aligned}
\nabla_{h^{(t)}} L 
\end{aligned}
$$

一样大。
在这个目标下，{Pascanu+al-ICML2013-small}提出以下正则项：
$$
\begin{aligned}
 \Omega = \sum_t \Bigg(  \frac{
 { ||(\nabla_{h^{(t)}} L) \frac{\partial h^{(t)}}{\partial h^{(t-1)}}}||}
 {||\nabla_{h^{(t)}} L||} -1 \Bigg)^2.
\end{aligned}
$$

计算这一梯度的正则项可能会出现困难，但{Pascanu+al-ICML2013-small}提出可以将后向传播向量$\nabla_{h^{(t)}} L$考虑为恒值作为近似（为了计算正则化的目的，没有必要通过它们向后传播）。
使用该正则项的实验表明，如果与标准的启发式截断（处理梯度爆炸）相结合，该正则项可以显著地增加RNN可以学习的依赖的跨度。
因为这种方法将RNN的动态保持在爆炸梯度的边缘，梯度截断特别重要。
如果没有梯度截断，梯度爆炸将阻碍学习的成功。

这种方法的一个主要弱点是，在处理数据冗余的任务时如语言模型，它并不像LSTM一样有效。