回忆一下\sec?中先验概率分布的概念。  
> **[warning]** 先验概率分布？  

这是一个模型参数的概率分布，它刻画了在我们看到数据之前我们认为什么样的模型是合理的信念。

 
先验被认为是强或者弱取决于先验中概率密度的集中程度。
弱先验具有较高的熵值，例如方差很大的高斯分布。这样的先验允许数据对于参数的改变具有或多或少的自由性。
**强先验具有较低的熵值**，例如方差很小的高斯分布。这样的先验在决定参数最终取值时起着更加积极的作用。

一个无限强的先验需要对一些参数取某些值的概率置零并且完全禁止对这些参数赋予这些值，无论数据对于这些参数值给出了多大的支持。

> **[success]**  
> 卷积引入的无限先验：  
> （1）隐藏单元的权重必须和它邻居的权重相同  
> （2）隐藏单元的小的空间连续的接受域内的权重以外，其余的权重都为零  
> （3）只包含局部连接关系并且对平移具有等变性  
> 池化引入的无限强先验：  
> （1）对少量平移的不变性  

我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。
这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。
这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外，其余的权重都为零。
<!-- %这样翻译对吗？其实这里加个图更好 -->
总之，我们可以把卷积的使用当作是对网络中一层的参数引入了一个无限强的先验概率分布。
这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性。
类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。  
> **[warning]** 平移不变性 VS 平移等变性？  

当然，把卷积神经网络当作一个具有无限强先验的全连接网络来实现会导致极大的计算浪费。
但把卷积神经网络想成具有无限强先验的全连接网络可以帮助我们更好地洞察卷积神经网络是如何工作的。

其中一个关键的洞察是卷积和池化可能导致欠拟合。
与任何其他先验类似，卷积和池化**只有当先验的假设合理且正确时才有用**。
如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差。
一些卷积网络结构~{cite?}为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上不使用。
当一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所利用的先验可能就不正确了。

> **[success]**  
> 池化的先验不合理：  
> 任务依赖于保存精确的空间信息  
> 卷积的先验不合理：  
> 务涉及到要对输入中相隔较远的信息进行合并  

另一个关键洞察是当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象。
其他不使用卷积的模型即使我们把图像中的所有像素点都置换后依然有可能进行学习。
对于许多图像数据集，还有一些分别的基准，有些是针对那些具有置换不变性并且必须通过学习发现拓扑结构的模型，还有一些是针对模型设计者将空间关系的知识植入了它们的模型。  
> **[warning]** 最后一段不懂  






