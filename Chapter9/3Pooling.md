卷积网络中一个典型层包含三级（如图9.7所示）。
在第一级中，这一层并行地计算多个卷积产生一组线性激活响应。
在第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。
这一级有时也被称为**探测级**（detector stage）。
在第三级中，我们使用**池化函数**(pooling function)来进一步调整这一层的输出。

> **[success]**  
> 卷积层：卷积操作、仿射变换  
> 探测层：非线性变换  
> 池化层：池化操作

　　  
> **[warning]** 卷积操作和仿射变换是什么关系？  

# 什么是池化层

池化函数**使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出**。
例如，**最大池化**（max pooling）函数给出相邻矩形区域内的最大值。
其他常用的池化函数包括相邻矩形区域内的平均值、$L^2$范数以及基于据中心像素距离的加权平均函数。

# 池化的特性：平移不变性  

不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似不变。  
> **[success]**  
> 池化层具有平移不变性。  
> 适用场合：只关心特征是否出现，不关心它在哪里  
> 不适用场合：特征的具体位置很重要。  

对于**平移的不变性**是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。
图9.8用了一个例子来说明这是如何实现的。
**局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时**。
例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。
但在一些其他领域，保存特征的具体位置却很重要。
例如当我们想要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它们是否相交。
 
使用池化可以看作是增加了一个**无限强的先验：这一层学得的函数必须具有对少量平移的不变性**。
当这个假设成立时，池化可以极大地提高网络的统计效率。

对空间区域进行池化产生了平移不变性，但当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性（如图9.9所示）。

# 池化的好处1

> **[success]** 对卷积进行池化的好处：  
> 平移不变  
> 旋转不变  
> 提高网络的计算效率  
> 减少参数存储的需求  

因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可能，我们可以通过综合池化区域的$k$个像素的统计特征而不是单个像素来实现。
图9.10给出了一个例子。
这种方法提高了网络的计算效率，因为下一层少了约$k$ 倍的输入。
当下一层的参数数目是关于那一层输入大小的函数时（例如当下一层是全连接的基于矩阵乘法的网络层时），这种对于输入规模的减小也可以提高统计效率并且减少对于参数的存储需求。
 
# 池化的好处2: 对不同大小的输入产生相同大小的输出

在很多任务中，池化对于处理不同大小的输入具有重要作用。  
> **[success]**  
> 池化的另一个作用：  
> 通过调整池化区域的偏置大小，使得不管什么大小的输入，都能得到相同大小的输出。  

例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小了。
例如，最终的池化层可能会输出四组综合统计特征，每组对应着图像的一个象限，而与图像的大小无关。

# 怎样选择池化函数

一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导。
将特征一起动态地池化也是可行的，例如，对于感兴趣特征的位置运行聚类算法。
这种方法对于每幅图像产生一个不同的池化区域集合。
另一种方法是先**学习**一个单独的池化结构，再应用到全部的图像中。  
> **[warning]** 这一段没看懂  
> 为什么说是“对于感兴趣特征的位置运行聚类算法”是“动态地池化”？  
> “先**学习**一个单独的池化结构，再应用到全部的图像中”是什么意思？  

池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，例如玻尔兹曼机和自编码器。
这些问题将在本书第3部分中当我们遇到这些类型的网络时进一步讨论。
卷积玻尔兹曼机中的池化出现在第20.6节。
一些可微网络中需要的在池化单元上进行的类逆运算将在第20.10.6节中讨论。

图9.11给出了一些使用卷积和池化操作的用于分类的完整卷积网络结构的例子。
