Dropout是一类正则化方法，计算方便，功能强大。  

# 原理

Bagging集成方法涉及训练多个模型，需要大量的时间和空间。  
Dropout提供**廉价**的Bagging近似，能够训练和评估指数级数量的神经网络。  

Dropout集成的是所有“从基础网络除去非输出单元后形成的子网络”。  
![](http://windmissing.github.io/images_for_gitbook/Bible-DeepLearning/12.png) 

# dropout和Bagging的区别

1. 在Bagging的情况下，所有模型都是独立的。
在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。  
2. 在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。
在Dropout的情况下，通常大部分模型都没有显式地被训练。取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。  

# 训练

每次随机从小批量中加载一个样本。  
将样本随机应用于dropout集中的一个子网络。  
[?]掩码那一段没看懂。  

# 预测

每个模型i产生一个概率分布$$p^{(i)}(y|x)$$  
集成测试由这些分布的[几何平均](https://baike.baidu.com/item/%E5%87%A0%E4%BD%95%E5%B9%B3%E5%9D%87%E6%95%B0/5557084?fr=aladdin)给出。  
[?]后面一大坨证明“为什么使用几何平均”和“怎样使用几何平均”。  

# 优点与缺点

优点：  
1. 计算方便。  
2. 不怎么限制适用的模型或训练过程。  

缺点：  
1. 减少了模型的有效容量。更大的模型和更多训练算法的迭代次数来弥补。  
2. 当数据集很大（过拟合不严重）时，dropout的效果不明显。  

[?]后面一大坨评论没看懂。  

-------------------------

Dropout提供了正则化一大类模型的方法，计算方便但功能强大。  
> <font color="red">
> [?] dropout能正则化“一大类模型”是什么意思？其他正则化方法都能正则化多种模型。
> </font>

为了有个初步的近似感受，我们可以认为Dropout是集成大量深层神经网络的实用Bagging方法。  
> [<font color="green"> Bagging </font>](https://windmising.gitbook.io/bible-deeplearning/0introduction-1/11bagging)

Bagging涉及训练多个模型，并在每个测试样本上评估多个模型。
当每个模型都是一个很大的神经网络时，这似乎是不切实际的，因为训练和评估这样的网络需要花费很多运行时间和内存。
通常我们只能集成五至十个神经网络，如Szegedy-et-al-arxiv2014集成了六个神经网络赢得ILSVRC，超过这个数量就会迅速变得难以处理。
Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。

具体而言，Dropout训练的集成包括所有从基础网络除去非输出单元后形成的子网络，如图7.6所示。
![](http://windmissing.github.io/images_for_gitbook/Bible-DeepLearning/12.png)  
最先进的神经网络基于一系列仿射变换和非线性变换，我们只需将一些单元的输出乘零就能有效地删除一个单元。
这个过程需要对模型（如径向基函数网络，单元的状态和参考值之间存在一定区别）进行一些修改。  
> <font color="red">
> [?] 怎样通过乘0删除一个单元？具体怎么修改？
> [?] 径向基函数网络
> [?] 单元的状态和参考值
> </font>

为了简单起见，我们在这里提出乘零的简单Dropout算法，但是它被简单修改后，可以与从网络中移除单元的其他操作结合使用。  
> <font color="red">
> [?] 移除单元的其他操作
> </font>

回想一下Bagging学习，我们定义$$k$$个不同的模型，从训练集有放回采样构造$$k$$个不同的数据集，然后在训练集$$i$$上训练模型$$i$$。
Dropout的目标是在指数级数量的神经网络上近似这个过程。
具体来说，在训练中使用Dropout时，我们会使用基于小批量产生较小步长的学习算法，如随机梯度下降等。
我们每次在小批量中加载一个样本，然后随机抽样应用于网络中所有输入和隐藏单元的不同二值掩码。  
> <font color="red">
> [?] 输入和隐藏单元的不同二值掩码
> </font>

对于每个单元，掩码是独立采样的。
掩码值为1的采样概率（导致包含一个单元）是训练开始前一个固定的超参数。
> <font color="green">
> 采样概率，当前训练时某个神经元不被删除的概率
> </font>

它不是模型当前参数值或输入样本的函数。
通常在每一个小批量训练的神经网络中，一个输入单元被包括的概率为$$0.8$$，一个隐藏单元被包括的概率为$$0.5$$。
然后，我们运行和之前一样的前向传播、反向传播以及学习更新。
图7.7说明了在Dropout下的前向传播。  

Dropout训练与Bagging训练不太一样。
在Bagging的情况下，所有模型都是独立的。
在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。
参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。  
> <font color="green">
> 这是以廉价的方式训练和评估指数级数量的神经网络的关键。
> </font>

在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。
在Dropout的情况下，通常大部分模型都没有显式地被训练，因为通常父神经网络会很大，以致于到宇宙毁灭都不可能采样完所有的子网络。
取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。
这些是仅有的区别。
除了这些，Dropout与Bagging算法一样。
例如，每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集。