Dropout是一类正则化方法，计算方便，功能强大。  

# 原理

Bagging集成方法涉及训练多个模型，需要大量的时间和空间。  
Dropout提供**廉价**的Bagging近似，能够训练和评估指数级数量的神经网络。  

Dropout集成的是所有“从基础网络除去非输出单元后形成的子网络”。  
![](https://github.com/windmissing/Bible-DeepLearning/raw/master/Chapter7/images/5.png)  

# 训练

每次随机从小批量中加载一个样本。  
将样本随机应用于dropout集中的一个子网络。  
[?]掩码那一段没看懂。  

# 预测

每个模型i产生一个概率分布$$p^{(i)}(y|x)$$  
集成测试由这些分布的[几何平均](TODO:现在上不了网，链接以后补上)给出。  
[?]后面一大坨证明“为什么使用几何平均”和“怎样使用几何平均”。  

# 优点与缺点

优点：  
1. 计算方便。  
2. 不怎么限制适用的模型或训练过程。  

缺点：  
1. 减少了模型的有效容量。更大的模型和更多训练算法的迭代次数来弥补。  
2. 当数据集很大（过拟合不严重）时，dropout的效果不明显。  

[?]后面一大坨评论没看懂。  