# 什么是L2正则化

> **[success]** L2正则化，别名：权重衰减，weight decay，L2参数惩罚，岭回归，Tikhonov 正则  

在第5.2节中我们已经看到过最简单而又最常见的参数范数惩罚，即通常被称为**权重衰减**（weight decay）的$$L^2$$参数范数惩罚。
这个正则化策略通过向目标函数添加一个正则项$$\Omega(\theta) = \frac{1}{2} ||{w}||_2^2$$，使权重更加接近原点。  
> **[info]** 更一般地，我们可以将参数正则化为接近空间中的任意特定点，令人惊讶的是这样也仍有正则化效果，但是特定点越接近真实值结果越好。
当我们不知道正确的值应该是正还是负时，零是有意义的默认值。
由于模型参数正则化为零的情况更为常见，我们将只探讨这种特殊情况。  

　　  
> 例如假设要训练的模型与另一个已经训练的模型非常相似，可以令这个模型的参数接近已经训练模型的参数。  

在其他学术圈，$$L^2$$也被称为岭回归或Tikhonov正则。

我们可以通过研究正则化后目标函数的梯度，洞察一些权重衰减的正则化表现。
为了简单起见，我们假定其中没有偏置参数，因此$$\theta$$就是$$w$$。  
> **[success]** 在下文中会出现$$\theta$$和w混用的情况，把所有$$\theta$$都看作是w就可以了。  

这样一个模型具有以下总的目标函数：  

$$
\begin{aligned}
  \tilde{J}(w;X, y) =\frac{\alpha}{2} w^\top w +  J(w;X, y)
\end{aligned}
$$

> **[success]**  
$$J(w;X, y)$$为基本代价函数。  
$$\tilde{J}(w;X, y)$$正则化的代价函数。  

与之对应的梯度为  

$$
\begin{aligned}
 \nabla_{w} \tilde{J}(w;X,y) =\alpha w +  \nabla_{w} J(w;X, y)
\end{aligned}
$$

使用单步梯度下降更新权重，即执行以下更新：  

$$
\begin{aligned}
 w \leftarrow w - \epsilon(\alpha w + \nabla_{w} J(w;X, y))
\end{aligned}
$$

换种写法就是：  

$$
\begin{aligned}
 w \leftarrow (1-\epsilon \alpha)w - \epsilon \nabla_{w} J(w;X, y)
\end{aligned}
$$

我们可以看到，加入权重衰减后会引起学习规则的修改，即**在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）**。  
> **[success]** 这就是“weight decay”的含义。  

这是单个步骤发生的变化。
但是，在训练的整个过程会发生什么呢？

# L2正则化的效果

我们进一步简化分析，令$$w^*$$为未正则化的目标函数取得最小训练误差时的权重向量，即$$w^* = \arg\min_{w} J(w)$$， 并在$$w^*$$的邻域对目标函数做二次近似。  
> **[success]**  
当w=w*时，J(w)取到最小值，此时$$\nabla_{w} J(w;X, y)=0$$。  
**问：什么叫“对目标函数做二次近似”?**  
答：将目标函数按照泰勒公式展开，并保留其中的函数项、一阶项和二阶项。即：  
$$
\begin{aligned}
J(\theta) = J(w) \approx & J(w^*) + \\
& (w - w^*)^\top \nabla_{w} J(w;X, y) + \\
& \frac{1}{2}(w - w^*)^\top H (w - w^*)
\end{aligned}
$$

如果目标函数确实是二次的(如以均方误差拟合线性回归模型的情况)，则该近似是完美的。  
> **[success]**  
在二阶近似的过程中，将展开项中高于二阶的项都省略掉的，因此只是近似。  
但如果目标函数本身是二次的，那么它展开后高于二阶的项都是0。省略的都是0项，因此说该近似是完美的。  

近似的$$\hat J(\theta)$$如下  

$$
\begin{aligned}
 \hat J(\theta) = J(w^*) + \frac{1}{2}(w - w^*)^\top H (w - w^*),
\end{aligned}
$$

> **[danger]** 注意这里是$$\hat J(\theta)$$，表示对基本代价函数$$J(\theta)$$的近似，与上文中表示带正则化项的代价函数$$\tilde J(\theta)$$是不同的。  

其中H是J在$$w^*$$处计算的Hessian矩阵(关于w)。  
> **[success]** [Hessian矩阵](https://windmising.gitbook.io/mathematics-basic-for-ml/xian-xing-dai-shu/special_matrix)  

因为$$w^*$$被定义为最优，即梯度消失为$$0$$，所以该二次近似中没有一阶项。
同样地，因为$$w^*$$是J的一个最优点，我们可以得出H是半正定的结论。
> **[success]** [半正定矩阵](https://windmising.gitbook.io/mathematics-basic-for-ml/xian-xing-dai-shu/special_matrix)   
根据Hessian矩阵的性质，当w处是极小值时，H矩阵是半正定的。  

当$$\hat J$$取得最小时，其梯度公式（7.7）  

$$
\begin{aligned}
  \nabla_{w} \hat{J}(w) = H (w - w^*)
\end{aligned}
$$

为0。  
> **[success]** 公式（7.7）的意思是，函数$$\hat J(w)$$在w处的梯度为$$H (w - w^*)$$。  

> **[warning]** $$\hat{J}(w)$$的梯度$$\nabla_{w} \hat{J}(w)$$是怎么推导出来的？  

为了研究权重衰减带来的影响，我们在式（7.7）中添加权重衰减的梯度。   
> **[success]** $$\hat J(\theta)$$近似$$J(\theta)$$，所以认为$$\nabla_{w} {J}(\theta) = \nabla_{w} \hat{J}(\theta)$$  
根据公式7.3，计算出$$\nabla_{w} \tilde {J}(\theta) = \alpha w + \nabla_{w} {J}(\theta) = \alpha w + H(w-w^*)$$  

现在我们探讨最小化正则化后的$$\hat J$$。
我们使用变量$$\tilde{w}$$表示此时的最优点:  

$$
\begin{aligned}
& \alpha \tilde{w} + H (\tilde{w} - w^*) = 0 \\
 \Rightarrow & (H + \alpha I) \tilde{w} = H w^* \\
 \Rightarrow & \tilde{w} = (H + \alpha I)^{-1} H w^* & (7.10)
 \end{aligned}
$$

当$$\alpha$$趋向于0时，正则化的解$$\tilde{w}$$会趋向$$w^*$$。 
那么当$$\alpha$$增加时会发生什么呢？
因为H是实对称的，所以我们可以将其分解为一个对角矩阵$$\Lambda$$和一组特征向量的标准正交基$$Q$$，并且有$$H = Q \Lambda Q^\top$$。  
> **[success]**  
[实对称矩阵的特征分解](https://windmising.gitbook.io/mathematics-basic-for-ml/xian-xing-dai-shu/eigendecomposition)   
[标准正交](https://windmising.gitbook.io/mathematics-basic-for-ml/xian-xing-dai-shu/special_matrix)  

将其应用于公式7.10，可得：  

$$
\begin{aligned}
 \tilde w &= ( Q \Lambda Q^\top + \alpha I)^{-1} Q \Lambda Q^\top w^* \\
                 &=  [ Q( \Lambda+ \alpha I)  Q^\top ]^{-1} Q \Lambda Q^\top w^* \\
                 &= Q( \Lambda+ \alpha I)^{-1} \Lambda Q^\top w^* & (7.13)
\end{aligned}
$$

> **[success]** 以上公式推导过程会用到标准正交矩阵的性质。  
Q是标准正交矩阵，因此满足$$Q^\top = Q^{-1}$$  

我们可以看到权重衰减的效果是沿着由H的特征向量所定义的轴缩放$$w^*$$。  
> **[success]** 
Q是H的特征向量组成的矩阵。因此公式7.13看作是对w*沿着H的特征向量所定义的轴缩放。  

具体来说，我们会根据$$\frac{\lambda_i}{\lambda_i + \alpha}$$因子缩放与$$H$$第$$i$$个特征向量对齐的$$w^*$$的分量。  
> **[success]** $$( \Lambda+ \alpha I)^{-1} \Lambda$$是对角矩阵，对角线上的值为$$\frac{\lambda_i}{\lambda_i + \alpha}$$，因此可以把对角线的值可以看作是对应特征向量的轴上的缩放比例。  

（不妨查看图2.3回顾这种缩放的原理）。  
> **[info]** 图2.3  
![](assets/images/Chapter7/1.png)

沿着H特征值较大的方向(如$$\lambda_i \gg \alpha$$)正则化的影响较小。
而$$\lambda_i \ll \alpha$$的分量将会收缩到几乎为零。
这种效应如图7.1所示。

**只有在显著减小目标函数方向上的参数会保留得相对完好。**  
> **[warning]** “显著减小目标函数方向”与"$$\lambda$$很大的方向"怎么联系？  

**在无助于目标函数减小的方向（对应Hessian矩阵较小的特征值）上改变参数不会显著增加梯度。**
这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。

# L2正则化在机器学习中的效果

目前为止，我们讨论了权重衰减对优化一个抽象通用的二次代价函数的影响。
这些影响具体是怎么和机器学习关联的呢？
我们可以研究线性回归，它的真实代价函数是二次的，因此我们可以使用相同的方法分析。
再次应用分析，我们会在这种情况下得到相同的结果，但这次我们使用训练数据的术语表述。
线性回归的代价函数是平方误差之和：  
> **[warning]** 对于线性回归来说，cross-entropy代价函数与MSE代价函数是等价的。  
其它情况呢？怎么计算cross entropy的代价函数。  

$$
\begin{aligned}
 (X w - y)^\top (X w - y)
\end{aligned}
$$

我们添加$$L^2$$正则项后，目标函数变为  

$$
\begin{aligned}
  (X w - y)^\top (X w - y) + \frac{1}{2}\alpha w^\top w.
\end{aligned}
$$

这将普通方程的解从  

$$
\begin{aligned}
  w = (X^\top X)^{-1} X^\top y & (7.16)
\end{aligned}
$$

变为  

$$
\begin{aligned}
   w = (X^\top X + \alpha I)^{-1} X^\top y  && (7.17)
\end{aligned}
$$

公式7.16中的矩阵$$X^\top X$$与协方差矩阵$$\frac{1}{m}X^\top X$$成正比。
$$L^2$$正则项将这个矩阵替换为公式7.17中的$$ (X^\top X + \alpha I)^{-1}$$
这个新矩阵与原来的是一样的，不同的仅仅是在对角加了$$\alpha$$。
这个矩阵的对角项对应每个输入特征的方差。
我们可以看到，**$$L^2$$正则化能让学习算法“感知”到输入x具有较高的方差，这会使得学习算法压缩那些与输出目标的协方差较小（相对增加方差）的特征的权重。**  
> **[warning]** [?]与输出目标的协方差?  


