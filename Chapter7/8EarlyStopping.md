随着迭代次数的增加，训练误差会越来越小，而验证误差在小到一定程度后会再次上升。  
![](https://github.com/windmissing/Bible-DeepLearning/raw/master/Chapter7/images/2.png)  

提前终止：  
如果在一定时间内验证误差没有改善就停止迭代。  
返回验证集上最好的参数而不是最新的参数。  

提前终止的代价：  
1. 时间代价  
训练期间要定期评估验证集。  
解决方法：在独立的机器/CPU/GPU上评估。  
2. 空间代价  
要保持最佳的参数副本。  
解决方法：可以存在磁盘上。  

提前终止的优点：  
1. 无需破坏学习动态就能很容易地使用提前终止。  
2. 可单独使用或与其他的正则化策略结合使用。  

[?]关于首次训练、第二轮训练这几段没看懂。  

提前终止的正则化原理：
[?]推导过程没看懂。  
结论是：提前终止类似于L2正则化。  