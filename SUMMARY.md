# Table of contents

* [Introduction](README.md)
* 第5章 机器学习基础
    * [5.1.2 性能度量P](Chapter5/1LearningAlgorithms/2PerformanceMeasure.md)
    * [5.2.1 没有免费午餐定理](Chapter5/2/1.md)
* [第6章 深度前馈网络](Chapter6/0Introduction.md)
    * [6.1 例子：学习XOR](Chapter6/1Examples.md)
    * [6.2 基于梯度的学习](Chapter6/2Gradient/0Introduction.md)
        * [6.2.1 代价函数](Chapter6/2Gradient/1Cost/0Introduction.md)
            * [6.2.1.1 使用最大似然学习条件分布](Chapter6/2Gradient/1Cost/1Likelihood.md)
        * [6.2.2 输出单元](Chapter6/2Gradient/2OutputUnit/0Introduction.md)
            * [6.2.2.1 用于高斯输出分布的线性神单元](Chapter6/2Gradient/2OutputUnit/1Linear.md)
            * [6.2.2.2 用于Bernoulli输出分布的sigmoid单元](Chapter6/2Gradient/2OutputUnit/2Sigmoid.md)
            * [6.2.2.3 用于Multinoulli输出分布的softmax单元](Chapter6/2Gradient/2OutputUnit/3Softmax.md)
    * [6.3 隐藏单元](Chapter6/3Hidden/0Introduction.md)
        * [6.3.1 ReLU及其扩展](Chapter6/3Hidden/1ReLU.md)
        * [6.3.2 logistic sigmoid与双曲正切函数](Chapter6/3Hidden/2SigmoidTanh.md)
    * [6.5 反向传播和其他的微分算法](Chapter6/5Backprop/0Introduction.md)
        * [6.5.1 计算图](Chapter6/5Backprop/1ComputationalGraphs.md)
        * [6.5.2 微积分中的链式法则](Chapter6/5Backprop/2ChainRule.md)
        * [6.5.3 递归地使用链式法则来实现反向传播](Chapter6/5Backprop/3Recursively.md)
        * [6.5.4 全连接MLP中的反向传播计算](Chapter6/5Backprop/4FullyConnectedMLP.md)
        * [6.5.5 符号到符号的导数](Chapter6/5Backprop/5Diveriation.md)
        * [6.5.6 一般化的反向传播](Chapter6/5Backprop/6-5-6.md)
        * [6.5.7 实例：用于MLP 训练的反向传播](Chapter6/5Backprop/7MLPTraining.md)
* [第7章 深度学习中的正则化](Chapter7/0Introduction.md)
    * [7.1 参数范数惩罚](Chapter7/1ParameterNormPenalties/0Introduction.md)
        * [7.1.1 L2参数正则化](Chapter7/1ParameterNormPenalties/1L2.md)
        * [7.1.2 L1参数正则化](Chapter7/1ParameterNormPenalties/2L1.md)
    * [7.8 提前终止](Chapter7/8EarlyStopping.md)
    * [7.11 Bagging 和其他集成方法](Chapter7/11Bagging.md)
    * [7.12 Dropout](Chapter7/12Dropout.md)
* [第8章 深度模型中的优化](Chapter8/0Optimization.md)
    * 8.2 神经网络优化中的挑战
        * [8.2.4 悬崖和梯度爆炸](Chapter8/2Challenges/4.md)
        * [8.2.5 长期依赖](Chapter8/2Challenges/5.md)
    * 8.3 基本算法
        * [8.3.1 随机梯度下降](Chapter8/3BasicAlgorithms/1SGD.md)
        * [8.3.2 动量](Chapter8/3BasicAlgorithms/2Momentum.md)
        * [8.3.3 Nesterov 动量](Chapter8/3BasicAlgorithms/3Nesterov.md)
    * 8.5 自适应学习率算法
        * [8.5.1 AdaGrad](Chapter8/5AdaptiveLearningRates/1AdaGrad.md)
        * [8.5.2 RMSProp](Chapter8/5AdaptiveLearningRates/2RMSProp.md)
        * [8.5.3 Adam](Chapter8/5AdaptiveLearningRates/3Adam.md)
* [第9章 卷积网络](Chapter9/0cnn.md)
    * [9.1 卷积运算](Chapter9/1Convolution.md)
    * [9.3 池化](Chapter9/Pooling.md)
    * [9.5 基本卷积函数的变体](Chapter9/Variant.md)
* [第10章 序列建模：循环和递归网络](Chapter10/Introduction.md)
    * [10.1 展开计算图](Chapter10/1Unfolding.md)
    * [10.2 循环神经网络](Chapter10/2RNN/0RNN.md)
        * [10.2.1 导师驱动过程和输出循环网络](Chapter10/2RNN/1TeacherForcing.md)
        * [10.2.2 计算循环神经网络的梯度](Chapter10/2RNN/2BPTT.md)
        * [10.2.3 作为有向图模型的循环网络](Chapter10/2RNN/3.md)
        * [10.2.4 基于上下文的RNN序列建模](Chapter10/2RNN/4.md)
    * [10.3 双向RNN](Chapter10/3Bidirectional.md)
    * [10.4 基于编码 - 解码的序列到序列架构](Chapter10/4EncoderDecoder.md)
    * [10.5 深度循环网络](Chapter10/5.md)
    * [10.6 递归神经网络](Chapter10/6.md)
    * [10.7 长期依赖的挑战](Chapter10/7.md)
    * [10.9 渗漏单元和其他多时间尺度的策略](Chapter10/9.md)
    * [10.10 长短期记忆和其他门控RNN](Chapter10/10Gate/0.md)
        * [10.10.1 LSTM](Chapter10/10Gate/1LSTM.md)
        * [10.10.2 其他门控RNN](Chapter10/10Gate/2OtherGates.md)
